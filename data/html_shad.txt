<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="yandex-verification" content="e52884e5ba57aa72"/><meta property="og:title" content="Школа анализа данных"/><meta property="og:description" content=""/><meta property="og:type" content="website"/><meta property="og:image" content="/static/social-share.ru.png"/><meta property="og:site_name" content="Школа анализа данных"/><link rel="shortcut icon" type="image/vnd.microsoft.icon" href="/dataschool/favicon.ico"/><link rel="icon" sizes="128x128" href="/static/favicon/128x128.png"/><link rel="icon" sizes="192x192" href="/static/favicon/192x192.png"/><link rel="apple-touch-icon" sizes="114x114" href="/static/favicon/114x114.png"/><link rel="apple-touch-icon" sizes="120x120" href="/static/favicon/120x120.png"/><link rel="apple-touch-icon" sizes="144x144" href="/static/favicon/144x144.png"/><link rel="apple-touch-icon" sizes="152x152" href="/static/favicon/152x152.png"/><link rel="apple-touch-icon" sizes="167x167" href="/static/favicon/167x167.png"/><link rel="apple-touch-icon" sizes="180x180" href="/static/favicon/180x180.png"/><meta name="viewport" content="width=device-width, maximum-scale=1.0, initial-scale=1.0"/><title>Разработка машинного обучения</title><meta name="description" content=""/><meta name="keywords" content=""/><meta property="og:title" content="Разработка машинного обучения"/><meta property="og:description" content=""/><meta name="next-head-count" content="22"/><link rel="preload" href="/_next/static/css/c548a0b8041c04ca6c11df514919242021982897_CSS.38a3c51a.chunk.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c548a0b8041c04ca6c11df514919242021982897_CSS.38a3c51a.chunk.css"/><link rel="preload" href="/_next/static/css/f0811fe9cf5fab03b83c5f353f05045bc09b9dd7_CSS.6632398b.chunk.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f0811fe9cf5fab03b83c5f353f05045bc09b9dd7_CSS.6632398b.chunk.css"/><link rel="preload" href="/_next/static/css/styles.a5ef8b6b.chunk.css" as="style"/><link rel="stylesheet" href="/_next/static/css/styles.a5ef8b6b.chunk.css"/><link rel="preload" href="/_next/static/chunks/main-6d2a64d1b2609e3ff0c4.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-a7c69ae57fb2c90b0ff0.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.5e03480598ad1120f007.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.fd125b3506c044f87988.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-8d36acfba5cc49432a11.js" as="script"/><link rel="preload" href="/_next/static/chunks/36a89214.0631567be2ca53cd9149.js" as="script"/><link rel="preload" href="/_next/static/chunks/ea88be26.7653e1aac39d91071425.js" as="script"/><link rel="preload" href="/_next/static/chunks/c548a0b8041c04ca6c11df514919242021982897.f6d5fe8a9488ddf15b3e.js" as="script"/><link rel="preload" href="/_next/static/chunks/c548a0b8041c04ca6c11df514919242021982897_CSS.e1033ba52a9d5b03d3fc.js" as="script"/><link rel="preload" href="/_next/static/chunks/f0811fe9cf5fab03b83c5f353f05045bc09b9dd7.5cd3f8f8c52901791310.js" as="script"/><link rel="preload" href="/_next/static/chunks/f0811fe9cf5fab03b83c5f353f05045bc09b9dd7_CSS.f747071f9f407dc1e291.js" as="script"/><link rel="preload" href="/_next/static/chunks/styles.51b0d17bb8fb84a0f8d2.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/course/%5Bid%5D-70b94a5e5d4d5db3c66f.js" as="script"/></head><body><div id="__next"><div></div><div class="layout layout_ru layout_specialization"><header class="header"><input type="checkbox" class="header__checkbox" id="mobile-menu-open"/><div class="header__container"><div class="header__row"><div class="logo-header"><a href="/dataschool" class="logo-header__link" title="Школа анализа данных"><h1 class="logo-header__site-name"><svg width="96" height="26" viewBox="0 0 96 26" class="logo-header__icon"><path data-name="Polygon 1" d="M15.546 0L31.1 25.231H0z" fill="#ffc546" fill-rule="evenodd"></path><circle cx="49.219" cy="13" r="12.937" fill="#e10b10"></circle><path d="M72.031 1H96v24H72.031z"></path></svg><span class="logo-header__text">Школа анализа данных</span></h1><div class="logo-header__title"><span class="logo-header__title-static">ШАД</span><span class="logo-header__title-full"><span class="logo-header__title-full-inset"></span></span><span class="logo-header__title-short"><span class="logo-header__title-short-inset"></span></span></div></a></div><div class="header__backdrop" role="button"></div><div class="header__controls"><nav class="top-menu"><a href="/dataschool/" class="top-menu__link top-menu__link_home" role="button"><span class="top-menu__link-text">Главная</span></a><a href="/dataschool/enroll" class="top-menu__link top-menu__link_enroll" role="button"><span class="top-menu__link-text">Поступающим</span></a><a href="/dataschool/education" class="top-menu__link top-menu__link_education top-menu__link_active" role="button"><span class="top-menu__link-text">Учёба</span></a><a href="/dataschool/universities" class="top-menu__link top-menu__link_universities" role="button"><span class="top-menu__link-text">Вузы</span></a><a href="/dataschool/life" class="top-menu__link top-menu__link_life" role="button"><span class="top-menu__link-text">Жизнь</span></a><a href="/dataschool/science" class="top-menu__link top-menu__link_science" role="button"><span class="top-menu__link-text">Наука</span></a><a href="/dataschool/online" class="top-menu__link top-menu__link_online" role="button"><span class="top-menu__link-text">Учебник и онлайн-курсы</span></a></nav></div><a href="" class="header__language-button"><span class="header__language-value">En</span></a><div class="header__toggle"><label class="header__toggle-label toggle-label" for="mobile-menu-open"><span>Меню</span></label></div></div></div></header><section class="academy-panel"><button class="academy-panel__toggle"><span class="academy-panel__toggle-inset"><svg width="192" height="21" viewBox="0 0 192 21" fill="none" class="academy-panel__toggle-icon"><path d="M13.215 18.018L8.565 0H4.676L0 18.018h2.643l.991-3.94h5.413l.966 3.94h3.202zm-8.97-6.377l2.109-8.348 2.084 8.348H4.244zm18.242 6.377h3.43l-4.854-7.025 4.27-6.015h-3.05l-4.27 6.015V4.978H14.99v13.04h3.025v-6.403l4.473 6.403zM36.21 9.126c0-3.163-1.576-4.356-4.778-4.356-2.008 0-3.583.649-4.498 1.193V8.53c.813-.623 2.592-1.297 4.142-1.297 1.449 0 2.11.519 2.11 1.919v.726h-.483c-4.626 0-6.684 1.555-6.684 4.2 0 2.644 1.576 4.122 3.914 4.122 1.779 0 2.541-.596 3.126-1.218h.127c.025.337.127.777.228 1.037h2.948a33.495 33.495 0 01-.152-3.19V9.127zm-3.024 5.781c-.382.57-1.093 1.037-2.16 1.037-1.271 0-1.907-.777-1.907-1.944 0-1.53 1.042-2.074 3.635-2.074h.432v2.981zm16.689.675h-1.347V4.978h-8.819v1.115c0 3.188-.203 7.31-1.27 9.489h-.941V21h2.796v-2.982h6.785V21h2.796v-5.418zm-4.372 0h-4.27c.84-1.97 1.068-5.523 1.068-7.778v-.39h3.202v8.168zm15.556 1.555v-2.515c-.94.648-2.516 1.219-3.99 1.219-2.211 0-3.05-1.063-3.177-3.241h7.294v-1.633c0-4.537-1.957-6.248-4.981-6.248-3.685 0-5.439 2.877-5.439 6.818 0 4.537 2.186 6.74 6.049 6.74 1.931 0 3.354-.518 4.244-1.14zm-4.905-9.981c1.5 0 1.957 1.27 1.957 2.903v.26h-4.22c.077-2.075.814-3.163 2.263-3.163zm17.961 10.862h2.948l-.051-13.04h-4.143l-2.77 8.685-2.49-8.685h-4.397v13.04h2.567V8.27l2.77 9.748h2.465l3.1-9.748v9.748zm5.477-13.04v13.04h2.618l4.676-8.062v8.063h2.948V4.977h-2.617L82.54 13.04V4.978h-2.948zm11.892 13.04h3.1l2.44-4.251h1.45v4.252h3.024V4.977h-4.626c-2.948 0-5.057 1.53-5.057 4.33 0 1.97.915 3.214 2.541 3.785l-2.872 4.925zm5.617-10.603h1.372v4.096h-1.448c-1.373 0-2.135-.596-2.135-2.1 0-1.426.89-1.996 2.211-1.996zm20.743 10.603h3.075V0h-4.473c-4.499 0-6.862 2.36-6.862 5.833 0 2.774 1.296 4.408 3.609 6.093l-4.016 6.092h3.33l4.472-6.818-1.55-1.063c-1.88-1.296-2.795-2.307-2.795-4.485 0-1.919 1.321-3.215 3.837-3.215h1.373v15.582zm40.383-.881v-2.515c-.941.648-2.516 1.219-3.99 1.219-2.211 0-3.05-1.063-3.177-3.241h7.294v-1.633c0-4.537-1.957-6.248-4.981-6.248-3.685 0-5.439 2.877-5.439 6.818 0 4.537 2.186 6.74 6.049 6.74 1.931 0 3.354-.518 4.244-1.14zm18.908 1.14c1.398 0 2.389-.259 3.126-.803V14.96c-.762.545-1.677.882-2.948.882-2.16 0-3.05-1.711-3.05-4.408 0-2.826 1.093-4.277 3.075-4.277 1.169 0 2.313.414 2.923.803V5.341c-.635-.363-1.754-.622-3.253-.622-3.863 0-5.871 2.825-5.871 6.792 0 4.356 1.957 6.767 5.998 6.767zm-46.61-13.3v5.16h-4.041v-5.16h-3.024v13.041h3.024v-5.444h4.041v5.444h3.025V4.979h-3.025zm16.672 10.604h-1.347V4.979h-8.819v1.115c0 3.188-.203 7.31-1.27 9.489h-.941V21h2.796v-2.982h6.785V21h2.796v-5.418zm20.687 2.438h3.431l-4.854-7.026 4.27-6.015h-3.05l-4.27 6.015V4.978h-3.024v13.04h3.024v-6.403l4.473 6.403zM153.322 7.155c1.499 0 1.957 1.27 1.957 2.903v.26h-4.219c.076-2.075.813-3.163 2.262-3.163zm-10.496 8.425h-4.27c.839-1.97 1.068-5.522 1.068-7.777v-.39h3.202v8.168zm49.022-6.454c0-3.163-1.576-4.356-4.778-4.356-2.008 0-3.584.649-4.499 1.193V8.53c.814-.623 2.593-1.297 4.143-1.297 1.448 0 2.109.519 2.109 1.919v.726h-.483c-4.625 0-6.684 1.555-6.684 4.2 0 2.644 1.576 4.122 3.914 4.122 1.779 0 2.542-.596 3.126-1.218h.127c.026.337.127.777.229 1.037H192a33.363 33.363 0 01-.152-3.19V9.127zm-3.025 5.781c-.381.57-1.093 1.037-2.16 1.037-1.271 0-1.906-.777-1.906-1.944 0-1.53 1.042-2.074 3.634-2.074h.432v2.981z" fill="currentColor"></path></svg></span></button><div class="academy-panel__backdrop" role="button"></div><div class="academy-panel__content"><button class="academy-panel__close"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" class="academy-panel__close-icon"><path fill-rule="evenodd" clip-rule="evenodd" d="M17.07 2.512a1.25 1.25 0 010 1.768L4.698 16.654a1.25 1.25 0 01-1.768-1.767L15.303 2.512a1.25 1.25 0 011.768 0z" fill="#000"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M2.93 2.512a1.25 1.25 0 000 1.768l12.373 12.374a1.25 1.25 0 001.768-1.767L4.697 2.512a1.25 1.25 0 00-1.768 0z" fill="#000"></path></svg></button><div class="academy-panel__content-inset"><iframe title=" " class="academy-panel__iframe" src="/dataschool/embed/academy-internal" frameBorder="0" name=" " width="100%" height="100%"></iframe></div></div></section><main class="content"><div class="specialization"><div class="specialization__left-menu-container"><div class="container"><div class="specialization__left-menu"><nav class="left-menu"><div class="left-menu__title">Специальности</div><a href="/dataschool/course/data-science" class="left-menu__link left-menu__link_data-science" role="button"><span class="left-menu__link-text">Data Science</span></a><a href="/dataschool/course/machine-learning" class="left-menu__link left-menu__link_machine-learning left-menu__link_active" role="button"><span class="left-menu__link-text">Разработка машинного обучения</span></a><a href="/dataschool/course/big-data-infrastructure" class="left-menu__link left-menu__link_big-data-infrastructure" role="button"><span class="left-menu__link-text">Инфраструктура больших данных</span></a><a href="/dataschool/course/data-analysis" class="left-menu__link left-menu__link_data-analysis" role="button"><span class="left-menu__link-text">Анализ данных в прикладных науках</span></a><a href="/dataschool/course/alternative-track" class="left-menu__link left-menu__link_alternative-track" role="button"><span class="left-menu__link-text">Альтернативный трек</span></a></nav></div></div></div><div class="container container_content"><div class="specialization__headline"><h1 class="heading-h1">Разработка машинного обучения</h1></div></div><div class="container container_content-inset"><div class="specialization__subtitle">Создание высокотехнологичных сервисов и приложений на основе машинного обучения.</div><div class="specialization__items"><div class="specialization__item"><div class="specialization__item-title">Для кого</div><div class="specialization__item-text">Это направление подойдёт тем, кому нравится программировать и создавать сервисы и приложения, которыми смогут пользоваться тысячи и миллионы людей.</div></div><div class="specialization__item"><div class="specialization__item-title">Чему мы учим</div><div class="specialization__item-text">Писать эффективный код, строить и оптимизировать промышленно-эффективные data-driven системы.</div></div><div class="specialization__item"><div class="specialization__item-title">Где применять эти знания</div><div class="specialization__item-text">В разработке высокотехнологичных  продуктов на основе машинного обучения.</div></div></div></div><div class="container container_content"><div class="specialization__program-title"><h2 class="heading-h2">Программа</h2></div></div><div class="container container_content-inset"><div class="specialization__intro"><div class="intro-section"><p>В течение семестра каждый учащийся должен успешно пройти как минимум три курса. Например, если в основной программе их два, то необходимо выбрать какой-нибудь из <a href="/dataschool/course/special-courses">спецкурсов</a>.</p><p>Знания проверяются в первую очередь с помощью домашних заданий — экзамены и контрольные проводятся только по некоторым предметам.</p></div></div><div class="specialization__program-items"><div class="specialization-semester"><div class="specialization-semester__title">Первый семестр</div><div class="specialization-semester__content"><div class="specialization-semester__content-required"><div class="specialization-semester__subtitle">Обязательные</div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Алгоритмы и структуры данных, часть 1</div></div><div class="specialization-course__content"><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Сложность и модели вычислений. Анализ учетных стоимостей (начало)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Анализ учетных стоимостей (окончание)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Алгоритмы Merge-Sort и Quick-Sort</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Порядковые статистики. Кучи (начало)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Кучи (окончание)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Хеширование</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Деревья поиска (начало)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Деревья поиска (продолжение)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">09</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Деревья поиска (окончание). Система непересекающихся множеств</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">10</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Задачи RMQ и LCA</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">11</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Структуры данных для геометрического поиска</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">12</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Задача о динамической связности в ненаправленном графе</div></div></div></div></div><div class="specialization-course__note"></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Обучение языку C++, часть 1</div></div><div class="specialization-course__content"><div class="specialization-course__description">С++ — мощный язык с богатым наследием. Тем, кто только ступил на путь освоения этого языка, очень просто заблудиться в изобилии техник и приёмов, созданных за последние 30 лет. Курс учит &quot;Modern C++&quot; — современному подмножеству языка (стандарты 11, 14 и 17). Много внимания уделяется инструментам и библиотекам — вещам которые не являются частью языка, но без которых не получится построить большой и сложный проект.</div><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Введение в С++.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Константы. Указатели и ссылки. Передача аргументов в функцию.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Классы.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Динамическое управление памятью.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Переменные, указатели и ссылки.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Управление памятью, умные указатели, RAII.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Стандартная библиотека шаблонов.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Наследование и виртуальные функции.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">09</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Обработка ошибок.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">10</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Паттерны проектирования.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">11</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Пространства имен Move‑семантика Perfect forwarding.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">12</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Представление структур и классов в памяти. Выравнивание данных. Указатели на члены/методы класса. Variadic templates.</div></div></div></div></div><div class="specialization-course__note"></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Машинное обучение, часть 1</div></div><div class="specialization-course__content"><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Основные понятия и примеры прикладных задач</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Метрические методы классификации</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Логические методы классификации и решающие деревья</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Градиентные линейные методы классификации</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Метод опорных векторов</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Многомерная линейная регрессия</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Нелинейная и непараметрическая регрессия, нестандартные функции потерь</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Прогнозирование временных рядов</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">09</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Байесовские методы классификации</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">10</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Логистическая регрессия</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">11</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Поиск ассоциативных правил</div></div></div></div></div><div class="specialization-course__note"></div></div></div></div></div><div class="specialization-semester"><div class="specialization-semester__title">Второй семестр</div><div class="specialization-semester__content"><div class="specialization-semester__content-required"><div class="specialization-semester__subtitle">Обязательные</div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Машинное обучение, часть 2</div></div><div class="specialization-course__content"><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Нейросетевые методы классификации и регрессии</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Композиционные методы классификации и регрессии</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Критерии выбора моделей и методы отбора признаков</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Ранжирование</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Обучение с подкреплением</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Обучение без учителя</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Задачи с частичным обучением</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Коллаборативная фильтрация</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">09</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Тематическое моделирование</div></div></div></div></div><div class="specialization-course__note"></div></div></div><div class="specialization-semester__content-optional"><div class="specialization-semester__subtitle">На выбор</div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Алгоритмы и структуры данных, часть 2</div></div><div class="specialization-course__content"><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Обход в ширину. Обход в глубину (начало)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Обход в глубину (продолжение)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Обход в глубину (окончание). 2-разрезы</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Поиск кратчайших путей (начало)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Поиск кратчайших путей (продолжение)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Минимальные остовные деревья</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Минимальные разрезы. Поиск подстрок (начало)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Поиск подстрок (продолжение)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">09</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Поиск подстрок (окончание)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">10</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Суффиксные деревья (начало)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">11</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Суффиксные деревья (окончание). Суффиксные массивы (начало)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">12</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Суффиксные массивы (окончание)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">13</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Длиннейшие общие подстроки. Приближенный поиск подстрок.</div></div></div></div></div><div class="specialization-course__note"><div class="specialization-course__note-text">или</div></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Язык Python</div></div><div class="specialization-course__content"><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Основы языка (часть 1)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Основы языка (часть 2)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Объектно-ориентированное программирование</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Обработка ошибок</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Оформление и тестирование кода</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Работа со строками</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Модель памяти</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Функциональное программирование</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">09</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Обзор библиотек (часть 1)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">10</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Обзор библиотек (часть 2)</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">11</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Параллельные вычисления в Python</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">12</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Расширенная работа с объектами</div></div></div></div></div><div class="specialization-course__note"><div class="specialization-course__note-text">или</div></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Обучение языку C++, часть 2</div></div><div class="specialization-course__content"><div class="specialization-course__description">Вторая часть курса по С++, в которой разбираются продвинутые темы и возможности языка.</div><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Многопоточное программирование. Синхронизация потоков с использованием мьютексов и условных переменных.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Атомарные переменные. Модель памяти С++. Примеры лок-фри структур данных.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Продвинутые техники мета-программирования в С++. Метафункции, SFINAE, концепты.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title"> Конкурентное программирование, взаимодействие с сетью.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Архитектура llvm. Работа с синтаксическим деревом разбора С++. Разработка инструментов для анализа С++ кода.</div></div></div></div></div><div class="specialization-course__note"></div></div></div></div></div><div class="specialization-semester"><div class="specialization-semester__title">Третий семестр</div><div class="specialization-semester__content"><div class="specialization-semester__content-optional"><div class="specialization-semester__subtitle">На выбор</div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Natural Language Processing</div></div><div class="specialization-course__content"><div class="specialization-course__description">&quot;NLP (Natural Language Processing) — это подмножество более широкой области AI, которая пытается научить компьютер понимать и обрабатывать сырые данные на естественном языке. Большая часть доступной сегодня информации — это не структурированные тексты. Нам как людям, конечно, не составляет труда их понять (если они на родном языке), но мы не способны обработать такое количество данных, какое могла бы обработать машина. Но как заставить машину понимать эти данные и, более того, извлекать из них какую-то информацию?
Несколько лет назад на открытии ACL (одной из основных, если не самой главной NLP-конференции) в своей президентской речи Marti Hearst призналась, что больше не может давать студентам свое любимое упражнение. На примере HAL 9000 (один из примеров искусственного интеллекта в научной фантастике) она спрашивала студентов, что машина может делать, как HAL, а что пока нет. Сейчас это уже не такое хорошее упражнение, так как почти все из этого сейчас под силу компьютеру. Поразительно, насколько быстро развивается область и как многого мы достигли.
В курсе мы постараемся дать вам понять и почувствовать, что происходит в мире. Какие задачи решаются, как это происходит; как некоторые статистические подходы (которым почти полностью были посвящены курсы по NLP ещё несколько лет назад) получают новую жизнь и новую интерпретацию в нейросетях, а какие постепенно отмирают. Мы покажем, что NLP это не набор пар (задача, решение), а общие идеи, которые проникают в разные задачи и отражают некоторую общую концепцию. Вы также узнаете, что происходит на практике, когда какие подходы более применимы.
Это то, что мы делаем, то, что мы любим, и мы готовы поделиться этим с вами :)&quot;</div><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title"><a href="https://lena-voita.github.io/nlp_course.html">https://lena-voita.github.io/nlp_course.html</a></div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title"><a href="https://github.com/yandexdataschool/nlp_course">https://github.com/yandexdataschool/nlp_course</a> </div></div></div></div></div><div class="specialization-course__note"><div class="specialization-course__note-text">или</div></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Компьютерное зрение</div></div><div class="specialization-course__content"><div class="specialization-course__description">&quot;Курс посвящен методам и алгоритмам компьютерного зрения, т.е. извлечения информации из изображений и видео. Рассмотрим основы обработки изображений, классификацию изображений, поиск изображений по содержанию, распознавание лиц, сегментацию изображений. Затем поговорим про алгоритмы обработки и анализа видео. Последняя часть курса посвящена трёхмерной реконструкции. Для большинства задач будем обсуждать сущестующие нейросетевые модели.
В курсе мы стараемся уделять внимание только наиболее современным методам, которые используются в настоящее время при решении практических и исследовательских задач. Курс в большей степени является практическим, а не теоретическим. Поэтому все лекции снабжены лабораторными и домашними работами, которые позволяют попробовать на практике большинство из рассматриваемых методов. Работы выполняются на языке Python, с использованием различных библиотек.&quot;</div><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Цифровое изображение и тональная коррекция</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Основы обработки изображений</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Совмещение изображений</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Классификация изображений и поиск похожих</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Сверточные нейросети для классификации и поиска похожих изображений</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Детектирование объектов</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Семантическая сегментация</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Перенос стиля и синтез изображений</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">09</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Распознавание видео</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">10</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Разреженная трёхмерная реконструкция</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">11</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Плотная трёхмерная реконструкция</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">12</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Реконструкция по одному кадру и облакам точек, параметрические модели</div></div></div></div></div><div class="specialization-course__note"><div class="specialization-course__note-text">или</div></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Байесовские методы в машинном обучении</div></div><div class="specialization-course__content"><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Байесовский подход к теории вероятностей</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Аналитический байесовский вывод</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Байесовский способ выбора модели</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Автоматическое определение релевантности</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Метод релевантных векторов для задачи классификации</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Вероятностные модели с латентными переменными</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Вариационный байесовский вывод</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Байесовская модель разделения смеси гауссиан</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">09</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Методы Монте-Карло с марковскими цепями</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">10</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Латентное размещение Дирихле</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">11</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Гауссовские процессы для регрессии и классификации</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">12</span><div class="specialization-course__item-container"><div class="specialization-course__item-title specialization-course__item-title_with-desc">Непараметрические байесовские методы</div></div></div></div></div><div class="specialization-course__note"></div></div></div></div></div><div class="specialization-semester"><div class="specialization-semester__title">Четвёртый семестр</div><div class="specialization-semester__content"><div class="specialization-semester__content-required"><div class="specialization-semester__subtitle">Обязательные</div><div class="specialization-course"><div class="specialization-course__header" role="button"><div class="specialization-course__title">ML Engineering Practice</div></div><div class="specialization-course__content"><div class="specialization-course__description">Курс представляет собой проектную работу по разработке ML-проектов в командах.</div><div class="specialization-course__list"></div></div><div class="specialization-course__note"></div></div><div class="specialization-course"><div class="specialization-course__header" role="button"><div class="specialization-course__title">ML Research Practice</div></div><div class="specialization-course__content"><div class="specialization-course__description">Курс представляет собой работу над командными исследовательскими проектам в области машинного обучения.</div><div class="specialization-course__list"></div></div><div class="specialization-course__note"></div></div></div><div class="specialization-semester__content-recommend"><div class="specialization-semester__subtitle">Рекомендуемые спецкурсы</div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Глубинное обучение</div></div><div class="specialization-course__content"><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title"><a href="https://github.com/yandexdataschool/Practical_DL">Материал курса</a></div></div></div></div></div><div class="specialization-course__note"></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Обучение с подкреплением</div></div><div class="specialization-course__content"><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title"><a href="https://github.com/yandexdataschool/Practical_RL">Материал курса</a></div></div></div></div></div><div class="specialization-course__note"></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Self Driving Cars</div></div><div class="specialization-course__content"><div class="specialization-course__description">В курсе рассматриваются основные компоненты беспилотных технологий: локализация, перцепция, предсказание, уровень поведения и планирование движения. Для каждой из компонент будут описаны основные подходы. Кроме того, студенты познакомятся с текущим состоянием рынка и технологическими вызовами.</div><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Обзор основных компонент и сенсоров беспилотного автомобиля. Уровни автономности. Drive by Wire. Беспилотные автомобили как бизнес-продукт. Способы оценки прогресса в создании беспилотников. Основы локализации: gnss, колесная одометрия, байесовские фильтры.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Методы лидарной локализации: ICP, NDT, LOAM. Введение в визуальный SLAM на примере ORB-SLAM. Постановка задачи GraphSLAM. Сведение задачи GraphSLAM к нелинейному МНК. Выбор правильной параметризации. Системы с особой структурой в GraphSLAM. Архитектурный подход: frontend и backend.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Задача распознавания в беспилотном автомобиле. Статические и динамические препятствия. Сенсоры для системы распознавания. Представление статических препятствий. Детекция статических препятствий по лидару (VSCAN, нейросетевые методы). Использование лидара совместно с изображениями для детекции статики (семантическая сегментация изображений, depth completion). Стерео камера и получение глубины из картинки. Stixel World.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Представление динамических препятствий в беспилотном автомобиле. Нейросетевые методы детекции объектов в 2D. Детекция на основе Bird-eye view представления лидарного облака. Использование лидара совместно с изображениями для детекции динамических препятствий. Детекция автомобилей в 3D на основе картинок (3D boxes fitting, CAD models). Детекция динамических препятствий на основе радара. Трекинг объектов.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Модели движения автомобиля: rear wheel, front wheel. Планирование траекторий. Понятие конфигурационного пространства. Графовые методы построения траекторий. Траектории, минимизирующие рывок. Оптимизационные методы построения траекторий.</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Планирование скорости в динамическом окружении. ST-планирование. Предсказание поведения других участников дорожного движения</div></div></div></div></div><div class="specialization-course__note"></div></div><div class="specialization-course"><div class="specialization-course__header specialization-course__header_with-content" role="button"><div class="specialization-course__title">Нейробайесовские методы</div></div><div class="specialization-course__content"><div class="specialization-course__description">Курс посвящен применению байесовских методов в глубинном обучении. На лекциях будет рассказано о применении вероятностного моделирования для построения порождающих моделей данных, использованию состязающихся сетей для приближенного вывода, моделированию неопределенности в параметрах нейронной сети и о некоторых открытых проблемах глубинного обучения.</div><div class="specialization-course__list"><div class="specialization-course__item"><span class="specialization-course__item-number">01</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Стохастический вариационный вывод</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">02</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Дважды стохастический вариационный вывод</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">03</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Вариационный автокодировщик, нормализующие потоки для вариационного вывода</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">04</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Методы снижения дисперсии в моделях со скрытыми переменными</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">05</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Оценка отношения плотностей распределений, применение на примере \alpha-GAN</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">06</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Байесовские нейронные сети</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">07</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Байесовское сжатие нейронных сетей</div></div></div><div class="specialization-course__item"><span class="specialization-course__item-number">08</span><div class="specialization-course__item-container"><div class="specialization-course__item-title">Полунеявный вариационный вывод</div></div></div></div></div><div class="specialization-course__note"></div></div></div></div></div></div></div></div></main><footer class="footer"><div class="footer__container"><div class="footer__logo-container"><div class="footer__logo-top-wrapper"><div class="footer__logo"><a href="https://yandex.ru" target="_blank" class="footer__logo-link" title="Яндекс" role="button"><svg width="89" height="25" viewBox="0 0 89 25" fill="none"><g clip-path="url(#yandex-ru_svg__a)"><path d="M84.853 21.769c1.746 0 2.983-.307 3.904-.967v-2.995c-.945.636-2.085 1.037-3.686 1.037-2.692 0-3.807-2.028-3.807-5.236 0-3.372 1.358-5.094 3.832-5.094 1.455 0 2.885.495 3.637.967V6.368c-.8-.425-2.182-.731-4.074-.731-4.826 0-7.324 3.372-7.324 8.09.025 5.164 2.474 8.042 7.518 8.042zM61.257 20.4v-2.995c-1.164.778-3.152 1.462-4.971 1.462-2.765 0-3.808-1.274-3.977-3.868h9.118v-1.934c0-5.401-2.45-7.43-6.208-7.43-4.608 0-6.79 3.42-6.79 8.114 0 5.4 2.74 8.019 7.541 8.019 2.401 0 4.172-.637 5.287-1.368zM26.652 5.92v6.132h-5.045V5.92h-3.783v15.542h3.783v-6.486h5.044v6.486h3.784V5.92H26.65zm20.83 12.618H45.81V5.92H34.8v1.32c0 3.798-.243 8.703-1.577 11.298h-1.212V25h3.492v-3.538h8.464V25h3.492v-6.462h.024zm25.828 2.924h4.292L71.54 13.09l5.336-7.17h-3.808l-5.335 7.17V5.92H63.95v15.543h3.783v-7.641l5.578 7.641zM55.122 8.514c1.867 0 2.45 1.51 2.45 3.467v.307h-5.263c.097-2.477 1.018-3.774 2.813-3.774zM42.026 18.538h-5.335c1.043-2.335 1.334-6.58 1.334-9.27v-.47h4.001v9.74z" fill="#000"></path><path d="M14.672 21.462h-3.856V2.901H9.094c-3.153 0-4.802 1.533-4.802 3.82 0 2.595 1.14 3.798 3.492 5.33l1.94 1.274-5.577 8.137H0l5.02-7.264C2.134 12.193.51 10.236.51 6.934.51 2.807 3.467 0 9.07 0h5.577l.025 21.462z" fill="currentColor"></path></g><defs><clipPath id="yandex-ru_svg__a"><path fill="#fff" d="M0 0h89v25H0z"></path></clipPath></defs></svg></a></div><div class="footer__contacts"><a class="footer__email" href="mailto:shad@yandex-team.ru">shad@yandex-team.ru</a></div></div><div class="copyright">© Школа анализа данных, 2007 — 2022</div></div><div class="footer__menu-container"><div class="footer__menu"><nav class="bottom-menu"><a href="/dataschool/" class="bottom-menu__link"><span class="bottom-menu__link-text">Главная</span></a><a href="/dataschool/enroll" class="bottom-menu__link"><span class="bottom-menu__link-text">Поступающим</span></a><a href="/dataschool/education" class="bottom-menu__link"><span class="bottom-menu__link-text">Учёба</span></a><a href="/dataschool/universities" class="bottom-menu__link"><span class="bottom-menu__link-text">Вузы</span></a><a href="/dataschool/life" class="bottom-menu__link"><span class="bottom-menu__link-text">Жизнь</span></a><a href="/dataschool/science" class="bottom-menu__link"><span class="bottom-menu__link-text">Наука</span></a><a href="/dataschool/online" class="bottom-menu__link"><span class="bottom-menu__link-text">Учебник и онлайн-курсы</span></a><a href="/dataschool/documents " class="bottom-menu__link"><span class="bottom-menu__link-text">Сведения об образовательной организации</span></a></nav></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"lang":"ru","data":{"fields":{"headline":"Разработка машинного обучения","subtitle":"Создание высокотехнологичных сервисов и приложений на основе машинного обучения.","activeMenu":"machine-learning","items":[{"title":"Для кого","text":"Это направление подойдёт тем, кому нравится программировать и создавать сервисы и приложения, которыми смогут пользоваться тысячи и миллионы людей."},{"title":"Чему мы учим","text":"Писать эффективный код, строить и оптимизировать промышленно-эффективные data-driven системы."},{"title":"Где применять эти знания","text":"В разработке высокотехнологичных  продуктов на основе машинного обучения."}],"program":{"title":"Программа","intro":"В течение семестра каждый учащийся должен успешно пройти как минимум три курса. Например, если в основной программе их два, то необходимо выбрать какой-нибудь из [спецкурсов] (/dataschool/course/special-courses).\n\nЗнания проверяются в первую очередь с помощью домашних заданий — экзамены и контрольные проводятся только по некоторым предметам.","items":[{"title":"Первый семестр","required":[{"title":"Алгоритмы и структуры данных, часть 1","description":"","teachers":"М. А. Бабенко","list":[{"listItemTitle":"Сложность и модели вычислений. Анализ учетных стоимостей (начало)","listItemDesc":"Основные ресурсы: память и время. О-символика. Примеры моделей вычисления: машина Тьюринга, RAM-машина. Сложность в среднем и худшем случаях. Пример: задача сортировки. Сортировка выбором. Теоретико-информационная нижняя оценка сложности. Разрешающие деревья. Нижняя оценка сложности в модели разрешающих деревьев. Массивы переменного размера: аддитивная и мультипликативная схемы реаллокации. Анализ мультипликативной схемы для массива переменного размера с помощью банковского метода."},{"listItemTitle":"Анализ учетных стоимостей (окончание)","listItemDesc":"Анализ учетных стоимостей операций: функция потенциала, истинные и учетные стоимости. Стеки и очереди. Реализация на основе массива переменного размера и на основе связанного списка. Моделирование очереди с помощью двух стеков. Задача о поддержании динамического максимума в стеке и очереди. Изменяемые (mutable) и неизменяемые (immutable) структуры данных. Структуры данных с хранением истории (persistent). Immutable-стек и immutable-очередь. Проблема множественного будущего при анализе учетных стоимостей в persistent-структурах."},{"listItemTitle":"Алгоритмы Merge-Sort и Quick-Sort","listItemDesc":"Понятие о методе «разделяй и властвуй». Алгоритм Merge-Sort. Слияние двух упорядоченных списков. Оценка сложности. K-way Merge-Sort для работы во внешней памяти. Сортировка слиянием без использования дополнительной памяти. Общая схема алгоритма Quick-Sort. Два варианта реализации Partition. Примеры неудачного выбора опорных элементов. Рандомизированный выбор опорного элемента. Сложность Quick-Sort в худшем и среднем случаях. Глубина рекурсии в худшем и среднем случаях. Элиминация хвостовой рекурсии. Задача об оптимальном дереве слияний. Коды Хаффмана. Слияние двух упорядоченных последовательностей различной длины. Теоретико-информационная нижняя оценка. Бинарный поиск \"от края\" (galloping)."},{"listItemTitle":"Порядковые статистики. Кучи (начало)","listItemDesc":"Нахождение порядковых статистик с помощью рандомизированной модификации алгоритма Quick-Sort. Линейность матожидания времени работы. Приближенные медианы. Выбор k-й порядковой статистики за линейное в худшем случае. Деревья со свойствами кучи. Почти полные бинарные деревья: нумерация вершин, навигация. Двоичная куча. Операция просеивания вниз и вверх. Реализация операций вставки, удаления и поиска минимума. Преобразование произвольного массива ключей в кучу (операция Make-Heap), линейность времени работы. Алгоритм сортировки Heap-Sort."},{"listItemTitle":"Кучи (окончание)","listItemDesc":"k-ичные кучи, зависимость сложности операций от выбора k. Биномиальные (binomial), левацкие (leftlist) и косые (skew) кучи."},{"listItemTitle":"Хеширование","listItemDesc":"Хеш-функции. Коллизии. Разрешение коллизий методом цепочек, методом последовательных проб и методом двойного хеширования. Гипотеза простого равномерного хеширования, оценка средней длины цепочки. Универсальные семейства хеш-функций, оценка средней длины цепочки. Построение универсального семейства для целочисленных ключей. Совершенные хеш-функции. Построение совершенной хеш-функции с помощью универсального семейства. Интерфейс множества с ошибками. Фильтр Блюма (Bloom filter). Оценка вероятности ложноположительного срабатывания. Интерфейс словаря с ошибками. Модификация фильтра Блюма (bloomier filter)."},{"listItemTitle":"Деревья поиска (начало)","listItemDesc":"Определение дерева поиска. Вставка и удаление элементов. Inorder-обход дерева. Красно черные деревья: определение и основные свойства. Реализация операций вставки для красно-черного дерева. Splay-деревья. Операция splay: zig, zig-zig и zig-zag шаги. Реализация операций вставки, удаления, слияния и разделения для splay-деревьев."},{"listItemTitle":"Деревья поиска (продолжение)","listItemDesc":"Декартовы деревья (дучи). Единственность декартова дерева для заданного набора различных ключей и приоритетов. Логарифмическая оценка матожидания высоты дучи.Операции слияния и разделения для дуч. Операции вставки и удаления элементов для дуч. Построение декартового дерева за линейное время при условии предварительной сортировки ключей."},{"listItemTitle":"Деревья поиска (окончание). Система непересекающихся множеств","listItemDesc":"B+ деревья: определения и основные свойства. Операции поиска, вставки и удаления для B+ деревьев. Системы непересекающихся множеств. Реализация с использованием леса. Ранги вершин, эвристика ранга. Логарифмическая оценка ранга через количество элементов. Рандомизированная ранговая эвристика. Эвристика сжатия путей. Оценка учетной стоимости операций (без доказательства)."},{"listItemTitle":"Задачи RMQ и LCA","listItemDesc":"Задачи RMQ (range minimum query) и LCA (least common ancestor). Сведение от задачи RMQ к задаче LCA, декартово дерево. Алгоритм Таржана для offline-версии задачи LCA. Простейшие алгоритмы для online-версии задачи LCA: полная и разреженная таблицы ответов. Алгоритм Фарах-Колтона-Бендера для к задаче ±1-RMQ. Сведение задачи LCA к задаче ±1-RMQ: эйлеров обход дерева."},{"listItemTitle":"Структуры данных для геометрического поиска","listItemDesc":"Location problem, stabbing problem. Деревья интервалов. Сведение системы интервалов к двумерной задаче. Задача поиска точек в коридоре. Priority search tree. Задача поиска точек в прямоугольнике. Дерево отрезков по координате X, упорядоченные по Y списки точек в каждой вершине. Сложность O(n log n) для построения и O(log^2 n) для запроса. Уменьшение времени поиска до O(log n). Задача одновременного поиска в наборе упорядоченных списков. Fractional cascading."},{"listItemTitle":"Задача о динамической связности в ненаправленном графе","listItemDesc":"Задача о динамической связности: вставки и удаления ребер, запросы о связности. Частный случай задачи для случая лесов. Дервья эйлеровых обходов: слияние и разделение. Использование амортизации и набора остовных лесов для решения со сложностью O(log^2 n)."}]},{"title":"Обучение языку C++, часть 1","description":"С++ — мощный язык с богатым наследием. Тем, кто только ступил на путь освоения этого языка, очень просто заблудиться в изобилии техник и приёмов, созданных за последние 30 лет. Курс учит \"Modern C++\" — современному подмножеству языка (стандарты 11, 14 и 17). Много внимания уделяется инструментам и библиотекам — вещам которые не являются частью языка, но без которых не получится построить большой и сложный проект.","teachers":"Ф. М. Короткий","list":[{"listItemTitle":"Введение в С++.","listItemDesc":""},{"listItemTitle":"Константы. Указатели и ссылки. Передача аргументов в функцию.","listItemDesc":""},{"listItemTitle":"Классы.","listItemDesc":""},{"listItemTitle":"Динамическое управление памятью.","listItemDesc":""},{"listItemTitle":"Переменные, указатели и ссылки.","listItemDesc":""},{"listItemTitle":"Управление памятью, умные указатели, RAII.","listItemDesc":""},{"listItemTitle":"Стандартная библиотека шаблонов.","listItemDesc":""},{"listItemTitle":"Наследование и виртуальные функции.","listItemDesc":""},{"listItemTitle":"Обработка ошибок.","listItemDesc":""},{"listItemTitle":"Паттерны проектирования.","listItemDesc":""},{"listItemTitle":"Пространства имен Move‑семантика Perfect forwarding.","listItemDesc":""},{"listItemTitle":"Представление структур и классов в памяти. Выравнивание данных. Указатели на члены/методы класса. Variadic templates.","listItemDesc":""}]},{"title":"Машинное обучение, часть 1","description":"","teachers":"К. В. Воронцов","list":[{"listItemTitle":"Основные понятия и примеры прикладных задач","listItemDesc":"Постановка задач обучения по прецедентам. Объекты и признаки. Типы шкал: бинарные, номинальные, порядковые, количественные. Типы задач: классификация, регрессия, прогнозирование, кластеризация. Примеры прикладных задач. Основные понятия: модель алгоритмов, метод обучения, функция потерь и функционал качества, принцип минимизации эмпирического риска, обобщающая способность, скользящий контроль. Методика экспериментального исследования и сравнения алгоритмов на модельных и реальных данных."},{"listItemTitle":"Метрические методы классификации","listItemDesc":"1. Метод ближайших соседей и его обобщения\n Метод ближайших соседей (kNN) и его обобщения. Подбор числа k по критерию скользящего контроля. Обобщённый метрический классификатор, понятие отступа. Метод потенциальных функций, градиентный алгоритм.\n 2. Отбор эталонов и оптимизация метрики\n Отбор эталонных объектов. Псевдокод: алгоритм СТОЛП. Функция конкурентного сходства, алгоритм FRiS-СТОЛП. Функционал полного скользящего контроля, формула быстрого вычисления для метода 1NN. Профиль компактности. Функция вклада объекта. Отбор эталонных объектов на основе минимизации функционала полного скользящего контроля. Эффективные структуры данных для быстрого поиска ближайших объектов в прямых и обратных окрестностях - метрические деревья. Проклятие размерности. Задача настройки весов признаков. Концепция вывода на основе прецедентов (CBR)."},{"listItemTitle":"Логические методы классификации и решающие деревья","listItemDesc":"1. Понятия закономерности и информативности\n Понятие логической закономерности. Эвристическое, статистическое, энтропийное определение информативности. Асимптотическая эквивалентность статистического и энтропийного определения. Сравнение областей эвристических и статистических закономерностей. Разновидности закономерностей: конъюнкции пороговых предикатов (гиперпараллелепипеды), синдромные правила, шары, гиперплоскости. Градиентный алгоритм синтеза конъюнкций, частные случаи: жадный алгоритм, стохастический локальный поиск, стабилизация, редукция. Бинаризация признаков. Алгоритм разбиения области значений признака на информативные зоны.\n 2. Решающие списки и деревья\n Решающий список. Жадный алгоритм синтеза списка. Решающее дерево. Псевдокод: жадный алгоритм ID3. Недостатки алгоритма и способы их устранения. Проблема переобучения. Редукция решающих деревьев: предредукция и постредукция. Преобразование решающего дерева в решающий список. Алгоритм LISTBB. Небрежные решающие деревья (oblivious decision trees)."},{"listItemTitle":"Градиентные линейные методы классификации","listItemDesc":"Линейный классификатор, непрерывные аппроксимации пороговой функции потерь. Связь с методом максимума правдоподобия. Метод стохастического градиента и частные случаи: адаптивный линейный элемент ADALINE, перcептрон Розенблатта, правило Хэбба. Теорема Новикова о сходимости. Доказательство теоремы Новикова. Эвристики: инициализация весов, порядок предъявления объектов, выбор величины градиентного шага, \"выбивание\" из локальных минимумов. Метод стохастического среднего градиента SAG. Проблема мультиколлинеарности и переобучения, редукция весов (weight decay). Байесовская регуляризация. Принцип максимума совместного правдоподобия данных и модели. Квадратичный (гауссовский) и лапласовский регуляризаторы. Настройка порога решающего правила по критерию числа ошибок I и II рода. Кривая ошибок (ROC curve). Алгоритм эффективного построения ROC-кривой. Градиентный метод максимизации AUC."},{"listItemTitle":"Метод опорных векторов","listItemDesc":" Оптимальная разделяющая гиперплоскость. Понятие зазора между классами (margin). Случаи линейной разделимости и отсутствия линейной разделимости. Связь с минимизацией регуляризованного эмпирического риска. Кусочно-линейная функция потерь. Задача квадратичного программирования и двойственная задача. Понятие опорных векторов. Рекомендации по выбору константы C. Функция ядра (kernel functions), спрямляющее пространство, теорема Мерсера. Способы конструктивного построения ядер. Примеры ядер. Обучение SVM методом активных ограничений. Алгоритм INCAS. Алгоритм SMO. Ню-SVM. SVM-регрессия. Метод релевантных векторов RVM. Регуляризации для отбора признаков: LASSO SVM, Elastic Net SVM, SFM, RFM."},{"listItemTitle":"Многомерная линейная регрессия","listItemDesc":"1. Нелинейная параметрическая регрессия\n Метод Ньютона-Рафсона, метод Ньютона-Гаусса. Обобщённая линейная модель (GLM). Одномерные нелинейные преобразования признаков: метод настройки с возвращениями (backfitting) Хасти-Тибширани.\n 2. Непараметрическая регрессия\n Сглаживание. Локально взвешенный метод наименьших квадратов и оценка Надарая-Ватсона. Выбор функции ядра. Выбор ширины окна сглаживания. Сглаживание с переменной шириной окна. Проблема выбросов и робастная непараметрическая регрессия. Алгоритм LOWESS. Доверительный интервал значения регрессии в точке. Проблемы \"проклятия размерности\" и выбора метрики.\n 3. Неквадратичные функции потерь\n Метод наименьших модулей. Квантильная регрессия. Пример прикладной задачи: прогнозирование потребительского спроса. Робастная регрессия, функция Мешалкина. SVM-регрессия."},{"listItemTitle":"Нелинейная и непараметрическая регрессия, нестандартные функции потерь","listItemDesc":"Многопоточные программы и GIL. Многопроцессорные программы."},{"listItemTitle":"Прогнозирование временных рядов","listItemDesc":"Задача прогнозирования временных рядов. Примеры приложений. Экспоненциальное скользящее среднее. Модель Хольта. Модель Тейла-Вейджа. Модель Хольта-Уинтерса. Адаптивная авторегрессионная модель. Следящий контрольный сигнал. Модель Тригга-Лича. Адаптивная селективная модель. Адаптивная композиция моделей. Адаптация весов с регуляризацией."},{"listItemTitle":"Байесовские методы классификации","listItemDesc":"1. Оптимальный байесовский классификатор\n Принцип максимума апостериорной вероятности. Функционал среднего риска. Ошибки I и II рода. Теорема об оптимальности байесовского классификатора. Оценивание плотности распределения: три основных подхода. Наивный байесовский классификатор.\n 2. Непараметрическое оценивание плотности\n Ядерная оценка плотности Парзена-Розенблатта. Одномерный и многомерный случаи. Метод парзеновского окна. Выбор функции ядра. Выбор ширины окна, переменная ширина окна. Робастное оценивание плотности. Непараметрический наивный байесовский классификатор.\n 3. Параметрическое оценивание плотности\n Нормальный дискриминантный анализ. Многомерное нормальное распределение, геометрическая интерпретация. Выборочные оценки параметров многомерного нормального распределения. Матричное дифференцирование. Вывод оценок параметров многомерного нормального распределения. Квадратичный дискриминант. Вид разделяющей поверхности. Подстановочный алгоритм, его недостатки и способы их устранения. Линейный дискриминант Фишера. Связь с методом наименьших квадратов. Проблемы мультиколлинеарности и переобучения. Регуляризация ковариационной матрицы. Робастное оценивание. Цензурирование выборки (отсев объектов-выбросов). Параметрический наивный байесовский классификатор. Жадное добавление признаков в линейном дискриминанте, метод редукции размерности Шурыгина.\n 4. Разделение смеси распределений\n Смесь распределений. EM-алгоритм: основная идея, понятие скрытых переменных. Вывод алгоритма без обоснования сходимости. Псевдокод EM-алгоритма. Критерий останова. Выбор начального приближения. Выбор числа компонентов смеси. Стохастический EM-алгоритм. Смесь многомерных нормальных распределений. Сеть радиальных базисных функций (RBF) и применение EM-алгоритма для её настройки. Сопоставление RBF-сети и SVM с гауссовским ядром."},{"listItemTitle":"Логистическая регрессия","listItemDesc":"Гипотеза экспоненциальности функций правдоподобия классов. Теорема о линейности байесовского оптимального классификатора. Оценивание апостериорных вероятностей классов с помощью сигмоидной функции активации. Логистическая регрессия. Принцип максимума правдоподобия и логарифмическая функция потерь. Метод стохастического градиента для логарифмической функции потерь. Сглаженное правило Хэбба. Метод наименьших квадратов с итеративным пересчётом весов (IRLS). Пример прикладной задачи: кредитный скоринг. Бинаризация признаков."},{"listItemTitle":"Поиск ассоциативных правил","listItemDesc":"Понятие ассоциативного правила и его связь с понятием логической закономерности. Примеры прикладных задач: анализ рыночных корзин, выделение терминов и тематики текстов. Алгоритм APriori. Два этапа: поиск частых наборов и рекурсивное порождение ассоциативных правил. Недостатки и пути усовершенствования алгоритма APriori. Алгоритм FP-growth. Понятия FP-дерева и условного FP-дерева. Два этапа поиска частых наборов в FP-growth: построение FP-дерева и рекурсивное порождение частых наборов. Общее представление о динамических и иерархических методах поиска ассоциативных правил."}]}],"optional":[],"recommend":[],"special":[]},{"title":"Второй семестр","required":[{"title":"Машинное обучение, часть 2","description":"","teachers":"К. В. Воронцов","list":[{"listItemTitle":"Нейросетевые методы классификации и регрессии","listItemDesc":"Биологический нейрон, модель МакКаллока-Питтса как линейный классификатор. Функции активации. Проблема полноты. Задача исключающего или. Полнота двухслойных сетей в пространстве булевых функций. Теоремы Колмогорова, Стоуна, Горбаня (без доказательства). Алгоритм обратного распространения ошибок. Эвристики: формирование начального приближения, ускорение сходимости, диагональный метод Левенберга-Марквардта. Проблема \"паралича\" сети. Метод послойной настройки сети. Подбор структуры сети: методы постепенного усложнения сети, оптимальное прореживание нейронных сетей (optimal brain damage). Введение в глубинное обучение."},{"listItemTitle":"Композиционные методы классификации и регрессии","listItemDesc":"1. Линейные композиции, бустинг\n Основные понятия: базовый алгоритм (алгоритмический оператор), корректирующая операция. Взвешенное голосование. Алгоритм AdaBoost. Экспоненциальная аппроксимация пороговой функции потерь. Процесс последовательного обучения базовых алгоритмов. Теорема о сходимости бустинга. Базовые алгоритмы в бустинге. Решающие пни. Варианты бустинга: GentleBoost, LogitBoost, BrownBoost, и другие. Алгоритм AnyBoost. Градиентный бустинг.\n 2. Эвристические и стохастические методы\n Простое голосование (комитет большинства). Эвристический алгоритм ComBoost. Идентификация нетипичных объектов (выбросов). Обобщение на большое число классов. Решающий список (комитет старшинства). Эвристический алгоритм. Стратегия выбора классов для базовых алгоритмов. Стохастические методы: бэггинг и метод случайных подпространств. Случайные леса.\n3. Нелинейные алгоритмические композиции\nСмесь алгоритмов, область компетентности алгоритма. Выпуклые функции потерь. Методы построения смесей: последовательный и иерархический. Построение смеси алгоритмов с помощью EM-подобного алгоритма. Нелинейная монотонная корректирующая операция. Случай классификации. Случай регрессии. Задача монотонизации выборки, изотонная регрессия."},{"listItemTitle":"Критерии выбора моделей и методы отбора признаков","listItemDesc":"1. Задачи оценивания и выбора моделей\n Внутренние и внешние критерии. Эмпирические и аналитические оценки функционала полного скользящего контроля. Скользящий контроль, разновидности эмпирических оценок скользящего контроля. Критерий непротиворечивости. Разновидности аналитических оценок. Регуляризация. Критерий Акаике (AIC). Байесовский информационный критерий (BIC). Статистические критерии: коэффициент детерминации, критерий Фишера, анализ регрессионных остатков. Агрегированные и многоступенчатые критерии.\n 2. Теория обобщающей способности\n Теория Вапника-Червоненкиса. Функционал равномерного отклонения частот ошибок. Функция роста, ёмкость семейства алгоритмов. Структурная минимизация риска. Оценка \"бритвы Оккама\". Радемахеровская сложность семейства алгоритмов. Комбинаторная теория переобучения. Функционал вероятности переобучения. Граф расслоения-связности. Оценки расслоения-связности.\n3. Методы отбора признаков\nСложность задачи отбора признаков. Полный перебор. Метод добавления и удаления, шаговая регрессия. Поиск в глубину, метод ветвей и границ. Усечённый поиск в ширину, многорядный итерационный алгоритм МГУА. Генетический алгоритм, его сходство с МГУА. Случайный поиск и Случайный поиск с адаптацией (СПА)."},{"listItemTitle":"Ранжирование","listItemDesc":"Постановка задачи обучения ранжированию. Примеры. Признаки в задаче ранжирования поисковой выдачи: текстовые, ссылочные, кликовые. TF-IDF. PageRank. Критерии качества ранжирования: Precision, MAP, AUC, DCG, NDCG, pFound. Ранговая классификация, OC-SVM. Попарный подход: RankingSVM, RankNet, LambdaRank."},{"listItemTitle":"Обучение с подкреплением","listItemDesc":"Задача о многоруком бандите. Жадные и эпсилон-жадные стратегии. Метод UCB (upper confidence bound). Стратегия Softmax. Среда для экспериментов. Адаптивные стратегии на основе скользящих средних. Метод сравнения с подкреплением. Метод преследования. Общая постановка задачи обучения с подкреплением. Ценность состояния среды. Ценность действия. Метод временных разностей. Метод SARSA. Метод Q-обучения. Многошаговое TD-прогнозирование. Обобщения методов временных разностей, SARSA, Q-обучения. Адаптивный полужадный метод VDBE."},{"listItemTitle":"Обучение без учителя","listItemDesc":"1. Кластеризация\n Постановка задачи кластеризации. Примеры прикладных задач. Типы кластерных структур. Графовые алгоритмы кластеризации. Выделение связных компонент. Кратчайший незамкнутый путь. Алгоритм ФОРЭЛ. Функционалы качества кластеризации. Статистические алгоритмы: EM-алгоритм и Алгоритм k средних (k-means).\n 2. Сети Кохонена\n Нейронная сеть Кохонена. Конкурентное обучение, стратегии WTA и WTM. Самоорганизующаяся карта Кохонена. Применение для визуального анализа данных. Искусство интерпретации карт Кохонена. Сети встречного распространения, их применение для кусочно-постоянной и гладкой аппроксимации функций.\n 3. Таксономия\n Агломеративная кластеризация, Алгоритм Ланса-Вильямса и его частные случаи. Алгоритм построения дендрограммы. Определение числа кластеров. Свойства сжатия/растяжения, монотонности и редуктивности. Псевдокод редуктивной версии алгоритма. Потоковые (субквадратичные) алгоритмы кластеризации."},{"listItemTitle":"Задачи с частичным обучением","listItemDesc":"Постановка задачи Semisupervised Learning, примеры приложений. Простые эвристические методы: self-training, co-training, co-learning. Адаптация алгоритмов кластеризации для решения задач с частичным обучением. Кратчайшиё незамкнутый путь. Алгоритм Ланса-Уильямса. Алгоритм k-средних. Трансдуктивный метод опорных векторов TSVM. Алгоритм Expectation-Regularization на основе многоклассовой регуляризированной логистической регрессии."},{"listItemTitle":"Коллаборативная фильтрация","listItemDesc":"Задачи коллаборативной фильтрации, транзакционные данные и матрица субъекты-объекты. Корреляционные методы user-based, item-based. Латентные методы на основе би-кластеризации. Алгоритм Брегмана. Латентные методы на основе матричных разложений. Метод главных компонент для разреженных данных. Метод стохастического градиента. Неотрицательные матричные разложения. Разреженный SVD. Метод чередующихся наименьших квадратов ALS.\n Трансакционные данные. Рекомендательные системы и задачи персонализации.\n Категориальные признаки с большим множеством значений.\n Методы восстановление пропущенных значений матрицы: на основе схожести столбцов/строк, на основе предположения о низком ранге матрицы.\n Взвешенное сингулярное разложение матрицы, алгоритм ALS, распределенная реализация.\n Факторизационные машины"},{"listItemTitle":"Тематическое моделирование","listItemDesc":"Задача тематического моделирования коллекции текстовых документов. Метод максимума правдоподобия. Униграммная модель документа. Применение метода множителей Лагранжа. Вероятностный латентный семантический анализ PLSA. ЕМ-алгоритм. Латентное размещение Дирихле LDA. Сглаженная частотная оценка условной вероятности. Аддитивная регуляризация тематических моделей. Регуляризованный EM-алгоритм, теорема о стационарной точке. Регуляризаторы разреживания, сглаживания, декоррелирования тем.\n Схема построения тематической модели. Способы оценки качества тематической модели: перплексия, когерентность тем, качество поиска. Обучение тематических моделей, подбор параметров модели и процедуры обучения. Стратегии разреживания тематических моделей. Коллокации и N-граммы, их учет в тематических моделях."}]}],"optional":[{"title":"Алгоритмы и структуры данных, часть 2","description":"","teachers":"М. А. Бабенко","list":[{"listItemTitle":"Обход в ширину. Обход в глубину (начало)","listItemDesc":"Графы: основные определения, обозначения и способы хранения. Обход в ширину и его использование для нахождения кратчайших путей. Обход в глубину и его основные свойства."},{"listItemTitle":"Обход в глубину (продолжение)","listItemDesc":"Анализ учетных стоимостей операций: функция потенциала, истинные и учетные стоимости. Стеки и очереди. Реализация на основе массива переменного размера и на основе связанного списка. Моделирование очереди с помощью двух стеков. Задача о поддержании динамического максимума в стеке и очереди. Изменяемые (mutable) и неизменяемые (immutable) структуры данных. Структуры данных с хранением истории (persistent). Immutable-стек и immutable-очередь. Проблема множественного будущего при анализе учетных стоимостей в persistent-структурах."},{"listItemTitle":"Обход в глубину (окончание). 2-разрезы","listItemDesc":"Отношение взаимной достижимости вершин. Компоненты сильной связности, конденсация. Ацикличность конденсации. Поиск сильно связных компонент с помощью обхода в глубину. Топологическая сортировка конденсации. 1- и 2-разрезы. Линейное пространство на ребрах графа, его размерность. Подпространство циклов графа, его размерность и базис. Подпространство разрезов графа. Разложение пространства в прямую ортогональную сумму подпространств циклов и разрезов. Генерация случайного равновероятного элемента в пространстве циклов. Рандомизированный поиск 2-разрезов на основе fingerprints."},{"listItemTitle":"Поиск кратчайших путей (начало)","listItemDesc":"Кратчайшие пути в графе, примеры функции длин. Оценки расстояний и их релаксация. Алгоритмы Форда-Беллмана и Флойда. Алгоритм Дейкстры."},{"listItemTitle":"Поиск кратчайших путей (продолжение)","listItemDesc":"Анализ сложности алгоритма Дейкстры. Использование бинарных и k-ичных куч. Двунаправленный алгоритм Дейкстры. Системы потенциалов в задаче о кратчайших путях. Критерий консервативности функции длин дуг в терминах наличия допустимого набора потенциалов. Алгоритм Джонсона для задачи APSPP при произвольных длинах дуг. Использование маяков (landmarks) для быстрого поиска кратчайших путей. Алгоритм ALT."},{"listItemTitle":"Минимальные остовные деревья","listItemDesc":"Задача об оптимальном остовном дереве. Хорошие множества, лемма о минимальном ребре в разрезе. Алгоритмы Краскала, Прима и Борувки. Оценки сложности."},{"listItemTitle":"Минимальные разрезы. Поиск подстрок (начало)","listItemDesc":"Задачи о минимальном глобальном разрезе и о минимальном s-t разрезе, их связь. Стягиванияя графа. Алгоритм Штёра-Вагнера. Строки: основные определения и обозначения. Задача поиска подстроки в строке: различные варианты постановки. Наивный алгоритм. Префикс-функция: определение и свойства. Построение префикс-функции за линейное время. Алгоритм Кнута-Морриса-Пратта."},{"listItemTitle":"Поиск подстрок (продолжение)","listItemDesc":"Z-функция: определение и использование в задаче поиска подстроки. Построение Z-функции за линейное время. Оптимизация поиска подстрок с помощью Z-функции по памяти. Использование Z-функции для задачи приближенного поиска подстрок с одной ошибкой за линейное время. Задача множественного поиска подстрок, ожидаемая асимптотика времени работы. Бор для набора слов: определение и способы представления. Префикс-функция на боре. Алгоритм Ахо--Корасик для множественного поиска подстрок."},{"listItemTitle":"Поиск подстрок (окончание)","listItemDesc":"Поиск подстрок: джокеры типа \"?\" и \"*\". Использование алгоритма Ахо-Корасик. Оценка сложности. Редукция по символу алфавита. Поиск подходящих позиций с использованием сверток. Связь сверток с умножением полиномов. Пара оптимизаций: бинаризация алфавита и сокращение длины текста с помощью двуслойного покрытия."},{"listItemTitle":"Суффиксные деревья (начало)","listItemDesc":"Быстрое преобразование Фурье, рекурсивный вариант алгоритма. Обратное преобразование Фурье. Суффиксный бор и суффиксное дерево, оценки размеров. Явные и неявные положения в суффиксном дереве. Суффиксные ссылки."},{"listItemTitle":"Суффиксные деревья (окончание). Суффиксные массивы (начало)","listItemDesc":"Общая схема алгоритма Укконена для построения сжатого суффиксного дерева за время, линейное по длине строки. Итерации и шаги алгоритма. Классификация шагов. Лемма о возможных переходах между шагами различных типов. Элиминация шагов типа 1: неявные пометки листовых дуг. Элиминация шагов типа 3: досрочное окончание итерации. Оценка количества шагов типа 2. Поиск положений для шагов типа 2: суффиксные ссылки. Прием «скачок по счетчику» для быстрого вычисления суффиксных ссылок. Лемма об изменении вершинной глубины при переходе по суффиксной ссылке. Доказательство линейности времени работы алгоритма Укконена. Суффиксные массивы. Поиск подстрок с помощью суффиксных массивов и бинарного поиска. Построение суффиксного массива за время O(n log n) методом маркировки (алгоритм Карпа-Миллера-Розенберга)."},{"listItemTitle":"Суффиксные массивы (окончание)","listItemDesc":"Ускорение поиска подстрок с помощью предварительно рассчитанных lcp-значенеий. Алгоритм Каркайнена-Сандерса построения суффиксного массива за линейное время. Наименьшие общие предки в суффиксном дереве, связь с LCP-функцией. LCP-массив: определение и свойства. Построение LCP-массива по суффиксному массиву за линейное время (алгоритм Арикавы-Аримуры-Касаи-Ли-Парка)."},{"listItemTitle":"Длиннейшие общие подстроки. Приближенный поиск подстрок.","listItemDesc":"Задача о длиннейшей общей подстроке. Решение за линейное время с помощью суффиксного дерева и суффиксного массива. Задача приближенного поиска подстрок в тексте. Формулировка в терминах расстояний по графу динамического программирования. Алгоритм Ландау-Вишкина: множества достижимых вершин и их границы. База и шаг алгоритма, использование LCP."}]},{"title":"Язык Python","description":"","teachers":"И. В. Белова","list":[{"listItemTitle":"Основы языка (часть 1)","listItemDesc":"Интерпретатор и его интерактивный режим. Динамическая типизация, базовые типы данных: числовые, str, list. Основные операторы, оператор print. Блоки кода, основные составные операторы: if, while, for. Основные встроенные функции. Создание пользовательских функций."},{"listItemTitle":"Основы языка (часть 2)","listItemDesc":"Выражения, приоритеты операторов. Работа с файлами. Тип dict, хэширование. Модули, оператор import, модуль sys."},{"listItemTitle":"Объектно-ориентированное программирование","listItemDesc":"Классы, объекты. Пользовательские классы, методы и члены. Конструктор класса. Перегрузка операторов. Объекты в Python."},{"listItemTitle":"Обработка ошибок","listItemDesc":"Исключения, их генерация и обработка. Пользовательские исключения. Освобождение ресурсов, менеджеры контекстов."},{"listItemTitle":"Оформление и тестирование кода","listItemDesc":"Документирование кода. Инструмент pydoc. Юнит-тестирование. Модуль unittest. Инструменты для тестирования. Инструменты pylint, pyflakes. Отладочные инструменты. Модули, создание модулей. Пространства имен. Исполнение модулей как скриптов."},{"listItemTitle":"Работа со\u0026nbsp;строками","listItemDesc":"Встроенные функции строк. Форматирование строк. Модуль string. Класс unicode, его функции. Кодировки и Unicode, кодирование файлов и исходного кода."},{"listItemTitle":"Модель памяти","listItemDesc":"Хранение объектов в памяти, сборщик мусора. Хранение объектов по ссылке и по значению. Изменяемые и неизменяемые объекты. Модуль copy."},{"listItemTitle":"Функциональное программирование","listItemDesc":"Обработка списков, функция map и др., лямбда-функции, распаковка списков и словарей. Расширенная обработка аргументов функций. Генераторы и \"ленивое\" исполнение. Управляющие исключения. Модуль itertools."},{"listItemTitle":"Обзор библиотек (часть 1)","listItemDesc":"Библиотеки для обработки аргументов командной строки. Системные библиотеки. Стандартные математические библиотеки. Регулярные выражения и модуль re."},{"listItemTitle":"Обзор библиотек (часть 2)","listItemDesc":"Библиотеки для работы с HTML/XML. Математические библиотеки: SciPy и др. Библиотека Tkinter."},{"listItemTitle":"Параллельные вычисления в Python","listItemDesc":"Многопоточные программы и GIL. Многопроцессорные программы."},{"listItemTitle":"Расширенная работа с объектами","listItemDesc":"Модификаторы доступа. Наследование, разрешение имен. Метаклассы. Объект type. Декораторы."}]},{"title":"Обучение языку C++, часть 2","description":"Вторая часть курса по С++, в которой разбираются продвинутые темы и возможности языка.","teachers":"Ф. М. Короткий","list":[{"listItemTitle":"Многопоточное программирование. Синхронизация потоков с использованием мьютексов и условных переменных.","listItemDesc":""},{"listItemTitle":"Атомарные переменные. Модель памяти С++. Примеры лок-фри структур данных.","listItemDesc":""},{"listItemTitle":"Продвинутые техники мета-программирования в С++. Метафункции, SFINAE, концепты.","listItemDesc":""},{"listItemTitle":" Конкурентное программирование, взаимодействие с сетью.","listItemDesc":""},{"listItemTitle":"Архитектура llvm. Работа с синтаксическим деревом разбора С++. Разработка инструментов для анализа С++ кода.","listItemDesc":""}]}],"recommend":[],"special":[]},{"title":"Третий семестр","required":[],"optional":[{"title":"Natural Language Processing","description":"\"NLP (Natural Language Processing) — это подмножество более широкой области AI, которая пытается научить компьютер понимать и обрабатывать сырые данные на естественном языке. Большая часть доступной сегодня информации — это не структурированные тексты. Нам как людям, конечно, не составляет труда их понять (если они на родном языке), но мы не способны обработать такое количество данных, какое могла бы обработать машина. Но как заставить машину понимать эти данные и, более того, извлекать из них какую-то информацию?\nНесколько лет назад на открытии ACL (одной из основных, если не самой главной NLP-конференции) в своей президентской речи Marti Hearst призналась, что больше не может давать студентам свое любимое упражнение. На примере HAL 9000 (один из примеров искусственного интеллекта в научной фантастике) она спрашивала студентов, что машина может делать, как HAL, а что пока нет. Сейчас это уже не такое хорошее упражнение, так как почти все из этого сейчас под силу компьютеру. Поразительно, насколько быстро развивается область и как многого мы достигли.\nВ курсе мы постараемся дать вам понять и почувствовать, что происходит в мире. Какие задачи решаются, как это происходит; как некоторые статистические подходы (которым почти полностью были посвящены курсы по NLP ещё несколько лет назад) получают новую жизнь и новую интерпретацию в нейросетях, а какие постепенно отмирают. Мы покажем, что NLP это не набор пар (задача, решение), а общие идеи, которые проникают в разные задачи и отражают некоторую общую концепцию. Вы также узнаете, что происходит на практике, когда какие подходы более применимы.\nЭто то, что мы делаем, то, что мы любим, и мы готовы поделиться этим с вами :)\"","teachers":"D. Talbot, Е. Войта","list":[{"listItemTitle":"https://lena-voita.github.io/nlp_course.html","listItemDesc":""},{"listItemTitle":"https://github.com/yandexdataschool/nlp_course ","listItemDesc":""}]},{"title":"Компьютерное зрение","description":"\"Курс посвящен методам и алгоритмам компьютерного зрения, т.е. извлечения информации из изображений и видео. Рассмотрим основы обработки изображений, классификацию изображений, поиск изображений по содержанию, распознавание лиц, сегментацию изображений. Затем поговорим про алгоритмы обработки и анализа видео. Последняя часть курса посвящена трёхмерной реконструкции. Для большинства задач будем обсуждать сущестующие нейросетевые модели.\nВ курсе мы стараемся уделять внимание только наиболее современным методам, которые используются в настоящее время при решении практических и исследовательских задач. Курс в большей степени является практическим, а не теоретическим. Поэтому все лекции снабжены лабораторными и домашними работами, которые позволяют попробовать на практике большинство из рассматриваемых методов. Работы выполняются на языке Python, с использованием различных библиотек.\"","teachers":"А. С. Конушин","list":[{"listItemTitle":"Цифровое изображение и тональная коррекция","listItemDesc":""},{"listItemTitle":"Основы обработки изображений","listItemDesc":""},{"listItemTitle":"Совмещение изображений","listItemDesc":""},{"listItemTitle":"Классификация изображений и поиск похожих","listItemDesc":""},{"listItemTitle":"Сверточные нейросети для классификации и поиска похожих изображений","listItemDesc":""},{"listItemTitle":"Детектирование объектов","listItemDesc":""},{"listItemTitle":"Семантическая сегментация","listItemDesc":""},{"listItemTitle":"Перенос стиля и синтез изображений","listItemDesc":""},{"listItemTitle":"Распознавание видео","listItemDesc":""},{"listItemTitle":"Разреженная трёхмерная реконструкция","listItemDesc":""},{"listItemTitle":"Плотная трёхмерная реконструкция","listItemDesc":""},{"listItemTitle":"Реконструкция по одному кадру и облакам точек, параметрические модели","listItemDesc":""}]},{"title":"Байесовские методы в машинном обучении","description":"","teachers":"Д. П. Ветров","list":[{"listItemTitle":"Байесовский подход к теории вероятностей","listItemDesc":"Классический (частотный) и байесовские подходы к теории вероятностей. Различия в интерпретации случайности и методе вывода (статистического оценивания параметров). Байесовское обобщение логического вывода. Примеры байесовских рассуждений."},{"listItemTitle":"Аналитический байесовский вывод","listItemDesc":"Сопряженные распределения. Получение сопряженного семейства по заданной функции правдоподобия на примере многомерной гауссианы с неизвестными параметрами. Экспоненциальный класс распределений. Вид сопряженного семейства распределений к заданному семейству из экспоненциального класса. Достаточные статистики. Вывод аналитических формулы для получения апостериорного распределения."},{"listItemTitle":"Байесовский способ выбора модели","listItemDesc":"Виды вероятностных моделей. Задачи выбора модели. Примеры. Различные подходы к решению. Бритва Оккама. Обоснованность модели. Геометрический смысл обоснованности. Выбор наиболее обоснованной вероятностной модели на примере парадокса Симпсона."},{"listItemTitle":"Автоматическое определение релевантности","listItemDesc":"Проверка статистических гипотез с помощью принципа наибольшей обоснованности. Соотношение с классическими методами проверки гипотез. Обобщенная линейная регрессия. Регуляризация как способ предотвращения переобучения. Настройка индивидуальных коэффициентов регуляризации с помощью максимизации обоснованности. Эффект автоопределения значимости (релевантности)."},{"listItemTitle":"Метод релевантных векторов для задачи классификации","listItemDesc":"Задача классификации. Логистическая регрессия, метод ее обучения. Приближение обоснованности модели логистической регрессии с помощью метода Лапласа. Возможные расширения метода релевантных векторов."},{"listItemTitle":"Вероятностные модели с латентными переменными","listItemDesc":"Обучение при скрытых переменных. Разложение неполного правдоподобия. Получение ЕМ-алгоритма в общем виде. Байесовское обобщение метода главных компонент, настройка обоснованности с помощью ЕМ-алгоритма. Расширения байесовской модели главных компонент."},{"listItemTitle":"Вариационный байесовский вывод","listItemDesc":"Дивергенция Кульбака Лейблера, ее основные свойства. Итерационные формулы для минимизации прямой дивергенции в рамках факторизованного приближения. Вариационный ЕМ-алгоритм. Общая схема различных подходов к оценке скрытых переменных в вероятностных моделях."},{"listItemTitle":"Байесовская модель разделения смеси гауссиан","listItemDesc":"Задача разделения смеси распределений как задача со скрытыми переменными. Вариационное приближение для оценки апостериорного распределения для смеси гауссиан."},{"listItemTitle":"Методы Монте-Карло с марковскими цепями","listItemDesc":"Методы получения случайных величин из заданного распределения. Генерация одномерных случайных величин методом обратной функции. Выборка с отказами. Выборка с перевзвешиванием. Основные понятия теории марковских цепей. Эргодичность. Уравнение детального баланса. Схема Метрополиса-Гастингса и схема Гиббса."},{"listItemTitle":"Латентное размещение Дирихле","listItemDesc":"Задача тематического моделирования текстов. Вероятностная модель порождения текстов. Обучение модели латентного размещения Дирихле с помощью вариационного ЕМ-алгоритма. Расширения модели латентного размещения Дирихле."},{"listItemTitle":"Гауссовские процессы для регрессии и классификации","listItemDesc":"Понятие о случайных процессах. Ковариационная функция. Гауссовские случайные процессы. Автоматический подбор ковариационной функции под обучающую выборку с помощью максимизации обоснованности модели. Применение гауссовских случайных процессов в задачах классификации."},{"listItemTitle":"Непараметрические байесовские методы","listItemDesc":"Случайные процесс Дирихле как случайная мера. Модель разделения дискретной смеси на основе процесса Дирихле. Применение процесса Дирихле в непараметрической кластеризации. Схема китайского ресторана и стик-брейкинга. Дальнейшие расширения модели процессов Дирихле (вложенные процессы Дирихле, иерархические процессы Дирихле). Примеры применения процессов Дирихле для решения прикладных задач."}]}],"recommend":[],"special":[]},{"title":"Четвёртый семестр","required":[{"title":"ML Engineering Practice","description":"Курс представляет собой проектную работу по разработке ML-проектов в командах.","teachers":"Ф.Г. Синицин","list":[]},{"title":"ML Research Practice","description":"Курс представляет собой работу над командными исследовательскими проектам в области машинного обучения.","teachers":"","list":[]}],"optional":[],"recommend":[{"title":"Глубинное обучение","description":"","teachers":"В. С. Лемпицкий","list":[{"listItemTitle":"[Материал курса] (https://github.com/yandexdataschool/Practical_DL)","listItemDesc":""}]},{"title":"Обучение с подкреплением","description":"","teachers":"Ф. Д. Ратников","list":[{"listItemTitle":"[Материал курса] (https://github.com/yandexdataschool/Practical_RL)","listItemDesc":""}]},{"title":"Self Driving Cars","description":"В курсе рассматриваются основные компоненты беспилотных технологий: локализация, перцепция, предсказание, уровень поведения и планирование движения. Для каждой из компонент будут описаны основные подходы. Кроме того, студенты познакомятся с текущим состоянием рынка и технологическими вызовами.","teachers":"А. В. Слесарев","list":[{"listItemTitle":"Обзор основных компонент и сенсоров беспилотного автомобиля. Уровни автономности. Drive by Wire. Беспилотные автомобили как бизнес-продукт. Способы оценки прогресса в создании беспилотников. Основы локализации: gnss, колесная одометрия, байесовские фильтры.","listItemDesc":""},{"listItemTitle":"Методы лидарной локализации: ICP, NDT, LOAM. Введение в визуальный SLAM на примере ORB-SLAM. Постановка задачи GraphSLAM. Сведение задачи GraphSLAM к нелинейному МНК. Выбор правильной параметризации. Системы с особой структурой в GraphSLAM. Архитектурный подход: frontend и backend.","listItemDesc":""},{"listItemTitle":"Задача распознавания в беспилотном автомобиле. Статические и динамические препятствия. Сенсоры для системы распознавания. Представление статических препятствий. Детекция статических препятствий по лидару (VSCAN, нейросетевые методы). Использование лидара совместно с изображениями для детекции статики (семантическая сегментация изображений, depth completion). Стерео камера и получение глубины из картинки. Stixel World.","listItemDesc":""},{"listItemTitle":"Представление динамических препятствий в беспилотном автомобиле. Нейросетевые методы детекции объектов в 2D. Детекция на основе Bird-eye view представления лидарного облака. Использование лидара совместно с изображениями для детекции динамических препятствий. Детекция автомобилей в 3D на основе картинок (3D boxes fitting, CAD models). Детекция динамических препятствий на основе радара. Трекинг объектов.","listItemDesc":""},{"listItemTitle":"Модели движения автомобиля: rear wheel, front wheel. Планирование траекторий. Понятие конфигурационного пространства. Графовые методы построения траекторий. Траектории, минимизирующие рывок. Оптимизационные методы построения траекторий.","listItemDesc":""},{"listItemTitle":"Планирование скорости в динамическом окружении. ST-планирование. Предсказание поведения других участников дорожного движения","listItemDesc":""}]},{"title":"Нейробайесовские методы","description":"Курс посвящен применению байесовских методов в глубинном обучении. На лекциях будет рассказано о применении вероятностного моделирования для построения порождающих моделей данных, использованию состязающихся сетей для приближенного вывода, моделированию неопределенности в параметрах нейронной сети и о некоторых открытых проблемах глубинного обучения.","teachers":"Д. П. Ветров","list":[{"listItemTitle":"Стохастический вариационный вывод","listItemDesc":""},{"listItemTitle":"Дважды стохастический вариационный вывод","listItemDesc":""},{"listItemTitle":"Вариационный автокодировщик, нормализующие потоки для вариационного вывода","listItemDesc":""},{"listItemTitle":"Методы снижения дисперсии в моделях со скрытыми переменными","listItemDesc":""},{"listItemTitle":"Оценка отношения плотностей распределений, применение на примере \\alpha-GAN","listItemDesc":""},{"listItemTitle":"Байесовские нейронные сети","listItemDesc":""},{"listItemTitle":"Байесовское сжатие нейронных сетей","listItemDesc":""},{"listItemTitle":"Полунеявный вариационный вывод","listItemDesc":""}]}],"special":[]}]},"seo":{"title":"Разработка машинного обучения","keywords":"","description":""}},"menu":{"title":"Специальности","menu":[{"page":"data-science","title":"Data Science","url":"/dataschool/course/data-science","text":"Решение задач по сбору и\u0026nbsp;анализу данных, возникающих в\u0026nbsp;большинстве современных сервисов: от\u0026nbsp;Алисы до\u0026nbsp;Яндекс.Погоды."},{"page":"machine-learning","title":"Разработка машинного обучения","url":"/dataschool/course/machine-learning","text":"Создание высокотехнологичных сервисов и\u0026nbsp;приложений на\u0026nbsp;основе машинного обучения."},{"page":"big-data-infrastructure","title":"Инфраструктура больших данных","url":"/dataschool/course/big-data-infrastructure","text":"Разработка систем хранения и\u0026nbsp;обработки больших данных."},{"page":"data-analysis","title":"Анализ данных в\u0026nbsp;прикладных науках","url":"/dataschool/course/data-analysis","text":"Применение data science в\u0026nbsp;областях, напрямую не связанных с IT: от\u0026nbsp;физики высоких энергий до\u0026nbsp;промышленного дизайна лекарств."},{"page":"alternative-track","title":"Альтернативный трек","url":"/dataschool/course/alternative-track","text":"Тем, у кого есть опыт в IT, но нет сильной математической базы, альтернативный трек поможет добрать недостающие знания по математике и влиться в одно из четырёх других направлений"}]}},"common":{"options":{"seo":{"title":"Школа анализа данных","description":"","keywords":"","verification":"e52884e5ba57aa72","thumbnail":"social-share.ru.png","metrikaID":550285}},"header":{"logoPrimaryTitle":{"home":{"full":"Школа анализа данных","staticPart":"Школа анализа данных","fullReplacement":"","shortReplacement":""},"inner":{"full":"Школа анализа данных","staticPart":"ШАД","fullReplacement":"","shortReplacement":""}},"title":"Меню","menu":[{"name":"home","title":"Главная","url":"/dataschool/","sideMenu":true},{"name":"enroll","title":"Поступающим","url":"/dataschool/enroll","sideMenu":true},{"name":"education","title":"Учёба","url":"/dataschool/education","sideMenu":true},{"name":"universities","title":"Вузы","url":"/dataschool/universities","sideMenu":true},{"name":"life","title":"Жизнь","url":"/dataschool/life","sideMenu":true},{"name":"science","title":"Наука","url":"/dataschool/science","sideMenu":true},{"name":"online","title":"Учебник и онлайн-курсы","url":"/dataschool/online","sideMenu":true},{"name":"documents","title":"Сведения об образовательной организации","url":"/dataschool/documents ","sideMenu":false}]},"footer":{"copyright":"\u0026copy; Школа анализа данных, 2007\u0026nbsp;\u0026mdash;\u0026nbsp;2022","logo":{"title":"Яндекс","link":"https://yandex.ru"},"email":"shad@yandex-team.ru","feedback":{"title":""}}}}},"page":"/course/[id]","query":{"lang":"ru","data":{"fields":{"headline":"Разработка машинного обучения","subtitle":"Создание высокотехнологичных сервисов и приложений на основе машинного обучения.","activeMenu":"machine-learning","items":[{"title":"Для кого","text":"Это направление подойдёт тем, кому нравится программировать и создавать сервисы и приложения, которыми смогут пользоваться тысячи и миллионы людей."},{"title":"Чему мы учим","text":"Писать эффективный код, строить и оптимизировать промышленно-эффективные data-driven системы."},{"title":"Где применять эти знания","text":"В разработке высокотехнологичных  продуктов на основе машинного обучения."}],"program":{"title":"Программа","intro":"В течение семестра каждый учащийся должен успешно пройти как минимум три курса. Например, если в основной программе их два, то необходимо выбрать какой-нибудь из [спецкурсов] (/dataschool/course/special-courses).\n\nЗнания проверяются в первую очередь с помощью домашних заданий — экзамены и контрольные проводятся только по некоторым предметам.","items":[{"title":"Первый семестр","required":[{"title":"Алгоритмы и структуры данных, часть 1","description":"","teachers":"М. А. Бабенко","list":[{"listItemTitle":"Сложность и модели вычислений. Анализ учетных стоимостей (начало)","listItemDesc":"Основные ресурсы: память и время. О-символика. Примеры моделей вычисления: машина Тьюринга, RAM-машина. Сложность в среднем и худшем случаях. Пример: задача сортировки. Сортировка выбором. Теоретико-информационная нижняя оценка сложности. Разрешающие деревья. Нижняя оценка сложности в модели разрешающих деревьев. Массивы переменного размера: аддитивная и мультипликативная схемы реаллокации. Анализ мультипликативной схемы для массива переменного размера с помощью банковского метода."},{"listItemTitle":"Анализ учетных стоимостей (окончание)","listItemDesc":"Анализ учетных стоимостей операций: функция потенциала, истинные и учетные стоимости. Стеки и очереди. Реализация на основе массива переменного размера и на основе связанного списка. Моделирование очереди с помощью двух стеков. Задача о поддержании динамического максимума в стеке и очереди. Изменяемые (mutable) и неизменяемые (immutable) структуры данных. Структуры данных с хранением истории (persistent). Immutable-стек и immutable-очередь. Проблема множественного будущего при анализе учетных стоимостей в persistent-структурах."},{"listItemTitle":"Алгоритмы Merge-Sort и Quick-Sort","listItemDesc":"Понятие о методе «разделяй и властвуй». Алгоритм Merge-Sort. Слияние двух упорядоченных списков. Оценка сложности. K-way Merge-Sort для работы во внешней памяти. Сортировка слиянием без использования дополнительной памяти. Общая схема алгоритма Quick-Sort. Два варианта реализации Partition. Примеры неудачного выбора опорных элементов. Рандомизированный выбор опорного элемента. Сложность Quick-Sort в худшем и среднем случаях. Глубина рекурсии в худшем и среднем случаях. Элиминация хвостовой рекурсии. Задача об оптимальном дереве слияний. Коды Хаффмана. Слияние двух упорядоченных последовательностей различной длины. Теоретико-информационная нижняя оценка. Бинарный поиск \"от края\" (galloping)."},{"listItemTitle":"Порядковые статистики. Кучи (начало)","listItemDesc":"Нахождение порядковых статистик с помощью рандомизированной модификации алгоритма Quick-Sort. Линейность матожидания времени работы. Приближенные медианы. Выбор k-й порядковой статистики за линейное в худшем случае. Деревья со свойствами кучи. Почти полные бинарные деревья: нумерация вершин, навигация. Двоичная куча. Операция просеивания вниз и вверх. Реализация операций вставки, удаления и поиска минимума. Преобразование произвольного массива ключей в кучу (операция Make-Heap), линейность времени работы. Алгоритм сортировки Heap-Sort."},{"listItemTitle":"Кучи (окончание)","listItemDesc":"k-ичные кучи, зависимость сложности операций от выбора k. Биномиальные (binomial), левацкие (leftlist) и косые (skew) кучи."},{"listItemTitle":"Хеширование","listItemDesc":"Хеш-функции. Коллизии. Разрешение коллизий методом цепочек, методом последовательных проб и методом двойного хеширования. Гипотеза простого равномерного хеширования, оценка средней длины цепочки. Универсальные семейства хеш-функций, оценка средней длины цепочки. Построение универсального семейства для целочисленных ключей. Совершенные хеш-функции. Построение совершенной хеш-функции с помощью универсального семейства. Интерфейс множества с ошибками. Фильтр Блюма (Bloom filter). Оценка вероятности ложноположительного срабатывания. Интерфейс словаря с ошибками. Модификация фильтра Блюма (bloomier filter)."},{"listItemTitle":"Деревья поиска (начало)","listItemDesc":"Определение дерева поиска. Вставка и удаление элементов. Inorder-обход дерева. Красно черные деревья: определение и основные свойства. Реализация операций вставки для красно-черного дерева. Splay-деревья. Операция splay: zig, zig-zig и zig-zag шаги. Реализация операций вставки, удаления, слияния и разделения для splay-деревьев."},{"listItemTitle":"Деревья поиска (продолжение)","listItemDesc":"Декартовы деревья (дучи). Единственность декартова дерева для заданного набора различных ключей и приоритетов. Логарифмическая оценка матожидания высоты дучи.Операции слияния и разделения для дуч. Операции вставки и удаления элементов для дуч. Построение декартового дерева за линейное время при условии предварительной сортировки ключей."},{"listItemTitle":"Деревья поиска (окончание). Система непересекающихся множеств","listItemDesc":"B+ деревья: определения и основные свойства. Операции поиска, вставки и удаления для B+ деревьев. Системы непересекающихся множеств. Реализация с использованием леса. Ранги вершин, эвристика ранга. Логарифмическая оценка ранга через количество элементов. Рандомизированная ранговая эвристика. Эвристика сжатия путей. Оценка учетной стоимости операций (без доказательства)."},{"listItemTitle":"Задачи RMQ и LCA","listItemDesc":"Задачи RMQ (range minimum query) и LCA (least common ancestor). Сведение от задачи RMQ к задаче LCA, декартово дерево. Алгоритм Таржана для offline-версии задачи LCA. Простейшие алгоритмы для online-версии задачи LCA: полная и разреженная таблицы ответов. Алгоритм Фарах-Колтона-Бендера для к задаче ±1-RMQ. Сведение задачи LCA к задаче ±1-RMQ: эйлеров обход дерева."},{"listItemTitle":"Структуры данных для геометрического поиска","listItemDesc":"Location problem, stabbing problem. Деревья интервалов. Сведение системы интервалов к двумерной задаче. Задача поиска точек в коридоре. Priority search tree. Задача поиска точек в прямоугольнике. Дерево отрезков по координате X, упорядоченные по Y списки точек в каждой вершине. Сложность O(n log n) для построения и O(log^2 n) для запроса. Уменьшение времени поиска до O(log n). Задача одновременного поиска в наборе упорядоченных списков. Fractional cascading."},{"listItemTitle":"Задача о динамической связности в ненаправленном графе","listItemDesc":"Задача о динамической связности: вставки и удаления ребер, запросы о связности. Частный случай задачи для случая лесов. Дервья эйлеровых обходов: слияние и разделение. Использование амортизации и набора остовных лесов для решения со сложностью O(log^2 n)."}]},{"title":"Обучение языку C++, часть 1","description":"С++ — мощный язык с богатым наследием. Тем, кто только ступил на путь освоения этого языка, очень просто заблудиться в изобилии техник и приёмов, созданных за последние 30 лет. Курс учит \"Modern C++\" — современному подмножеству языка (стандарты 11, 14 и 17). Много внимания уделяется инструментам и библиотекам — вещам которые не являются частью языка, но без которых не получится построить большой и сложный проект.","teachers":"Ф. М. Короткий","list":[{"listItemTitle":"Введение в С++.","listItemDesc":""},{"listItemTitle":"Константы. Указатели и ссылки. Передача аргументов в функцию.","listItemDesc":""},{"listItemTitle":"Классы.","listItemDesc":""},{"listItemTitle":"Динамическое управление памятью.","listItemDesc":""},{"listItemTitle":"Переменные, указатели и ссылки.","listItemDesc":""},{"listItemTitle":"Управление памятью, умные указатели, RAII.","listItemDesc":""},{"listItemTitle":"Стандартная библиотека шаблонов.","listItemDesc":""},{"listItemTitle":"Наследование и виртуальные функции.","listItemDesc":""},{"listItemTitle":"Обработка ошибок.","listItemDesc":""},{"listItemTitle":"Паттерны проектирования.","listItemDesc":""},{"listItemTitle":"Пространства имен Move‑семантика Perfect forwarding.","listItemDesc":""},{"listItemTitle":"Представление структур и классов в памяти. Выравнивание данных. Указатели на члены/методы класса. Variadic templates.","listItemDesc":""}]},{"title":"Машинное обучение, часть 1","description":"","teachers":"К. В. Воронцов","list":[{"listItemTitle":"Основные понятия и примеры прикладных задач","listItemDesc":"Постановка задач обучения по прецедентам. Объекты и признаки. Типы шкал: бинарные, номинальные, порядковые, количественные. Типы задач: классификация, регрессия, прогнозирование, кластеризация. Примеры прикладных задач. Основные понятия: модель алгоритмов, метод обучения, функция потерь и функционал качества, принцип минимизации эмпирического риска, обобщающая способность, скользящий контроль. Методика экспериментального исследования и сравнения алгоритмов на модельных и реальных данных."},{"listItemTitle":"Метрические методы классификации","listItemDesc":"1. Метод ближайших соседей и его обобщения\n Метод ближайших соседей (kNN) и его обобщения. Подбор числа k по критерию скользящего контроля. Обобщённый метрический классификатор, понятие отступа. Метод потенциальных функций, градиентный алгоритм.\n 2. Отбор эталонов и оптимизация метрики\n Отбор эталонных объектов. Псевдокод: алгоритм СТОЛП. Функция конкурентного сходства, алгоритм FRiS-СТОЛП. Функционал полного скользящего контроля, формула быстрого вычисления для метода 1NN. Профиль компактности. Функция вклада объекта. Отбор эталонных объектов на основе минимизации функционала полного скользящего контроля. Эффективные структуры данных для быстрого поиска ближайших объектов в прямых и обратных окрестностях - метрические деревья. Проклятие размерности. Задача настройки весов признаков. Концепция вывода на основе прецедентов (CBR)."},{"listItemTitle":"Логические методы классификации и решающие деревья","listItemDesc":"1. Понятия закономерности и информативности\n Понятие логической закономерности. Эвристическое, статистическое, энтропийное определение информативности. Асимптотическая эквивалентность статистического и энтропийного определения. Сравнение областей эвристических и статистических закономерностей. Разновидности закономерностей: конъюнкции пороговых предикатов (гиперпараллелепипеды), синдромные правила, шары, гиперплоскости. Градиентный алгоритм синтеза конъюнкций, частные случаи: жадный алгоритм, стохастический локальный поиск, стабилизация, редукция. Бинаризация признаков. Алгоритм разбиения области значений признака на информативные зоны.\n 2. Решающие списки и деревья\n Решающий список. Жадный алгоритм синтеза списка. Решающее дерево. Псевдокод: жадный алгоритм ID3. Недостатки алгоритма и способы их устранения. Проблема переобучения. Редукция решающих деревьев: предредукция и постредукция. Преобразование решающего дерева в решающий список. Алгоритм LISTBB. Небрежные решающие деревья (oblivious decision trees)."},{"listItemTitle":"Градиентные линейные методы классификации","listItemDesc":"Линейный классификатор, непрерывные аппроксимации пороговой функции потерь. Связь с методом максимума правдоподобия. Метод стохастического градиента и частные случаи: адаптивный линейный элемент ADALINE, перcептрон Розенблатта, правило Хэбба. Теорема Новикова о сходимости. Доказательство теоремы Новикова. Эвристики: инициализация весов, порядок предъявления объектов, выбор величины градиентного шага, \"выбивание\" из локальных минимумов. Метод стохастического среднего градиента SAG. Проблема мультиколлинеарности и переобучения, редукция весов (weight decay). Байесовская регуляризация. Принцип максимума совместного правдоподобия данных и модели. Квадратичный (гауссовский) и лапласовский регуляризаторы. Настройка порога решающего правила по критерию числа ошибок I и II рода. Кривая ошибок (ROC curve). Алгоритм эффективного построения ROC-кривой. Градиентный метод максимизации AUC."},{"listItemTitle":"Метод опорных векторов","listItemDesc":" Оптимальная разделяющая гиперплоскость. Понятие зазора между классами (margin). Случаи линейной разделимости и отсутствия линейной разделимости. Связь с минимизацией регуляризованного эмпирического риска. Кусочно-линейная функция потерь. Задача квадратичного программирования и двойственная задача. Понятие опорных векторов. Рекомендации по выбору константы C. Функция ядра (kernel functions), спрямляющее пространство, теорема Мерсера. Способы конструктивного построения ядер. Примеры ядер. Обучение SVM методом активных ограничений. Алгоритм INCAS. Алгоритм SMO. Ню-SVM. SVM-регрессия. Метод релевантных векторов RVM. Регуляризации для отбора признаков: LASSO SVM, Elastic Net SVM, SFM, RFM."},{"listItemTitle":"Многомерная линейная регрессия","listItemDesc":"1. Нелинейная параметрическая регрессия\n Метод Ньютона-Рафсона, метод Ньютона-Гаусса. Обобщённая линейная модель (GLM). Одномерные нелинейные преобразования признаков: метод настройки с возвращениями (backfitting) Хасти-Тибширани.\n 2. Непараметрическая регрессия\n Сглаживание. Локально взвешенный метод наименьших квадратов и оценка Надарая-Ватсона. Выбор функции ядра. Выбор ширины окна сглаживания. Сглаживание с переменной шириной окна. Проблема выбросов и робастная непараметрическая регрессия. Алгоритм LOWESS. Доверительный интервал значения регрессии в точке. Проблемы \"проклятия размерности\" и выбора метрики.\n 3. Неквадратичные функции потерь\n Метод наименьших модулей. Квантильная регрессия. Пример прикладной задачи: прогнозирование потребительского спроса. Робастная регрессия, функция Мешалкина. SVM-регрессия."},{"listItemTitle":"Нелинейная и непараметрическая регрессия, нестандартные функции потерь","listItemDesc":"Многопоточные программы и GIL. Многопроцессорные программы."},{"listItemTitle":"Прогнозирование временных рядов","listItemDesc":"Задача прогнозирования временных рядов. Примеры приложений. Экспоненциальное скользящее среднее. Модель Хольта. Модель Тейла-Вейджа. Модель Хольта-Уинтерса. Адаптивная авторегрессионная модель. Следящий контрольный сигнал. Модель Тригга-Лича. Адаптивная селективная модель. Адаптивная композиция моделей. Адаптация весов с регуляризацией."},{"listItemTitle":"Байесовские методы классификации","listItemDesc":"1. Оптимальный байесовский классификатор\n Принцип максимума апостериорной вероятности. Функционал среднего риска. Ошибки I и II рода. Теорема об оптимальности байесовского классификатора. Оценивание плотности распределения: три основных подхода. Наивный байесовский классификатор.\n 2. Непараметрическое оценивание плотности\n Ядерная оценка плотности Парзена-Розенблатта. Одномерный и многомерный случаи. Метод парзеновского окна. Выбор функции ядра. Выбор ширины окна, переменная ширина окна. Робастное оценивание плотности. Непараметрический наивный байесовский классификатор.\n 3. Параметрическое оценивание плотности\n Нормальный дискриминантный анализ. Многомерное нормальное распределение, геометрическая интерпретация. Выборочные оценки параметров многомерного нормального распределения. Матричное дифференцирование. Вывод оценок параметров многомерного нормального распределения. Квадратичный дискриминант. Вид разделяющей поверхности. Подстановочный алгоритм, его недостатки и способы их устранения. Линейный дискриминант Фишера. Связь с методом наименьших квадратов. Проблемы мультиколлинеарности и переобучения. Регуляризация ковариационной матрицы. Робастное оценивание. Цензурирование выборки (отсев объектов-выбросов). Параметрический наивный байесовский классификатор. Жадное добавление признаков в линейном дискриминанте, метод редукции размерности Шурыгина.\n 4. Разделение смеси распределений\n Смесь распределений. EM-алгоритм: основная идея, понятие скрытых переменных. Вывод алгоритма без обоснования сходимости. Псевдокод EM-алгоритма. Критерий останова. Выбор начального приближения. Выбор числа компонентов смеси. Стохастический EM-алгоритм. Смесь многомерных нормальных распределений. Сеть радиальных базисных функций (RBF) и применение EM-алгоритма для её настройки. Сопоставление RBF-сети и SVM с гауссовским ядром."},{"listItemTitle":"Логистическая регрессия","listItemDesc":"Гипотеза экспоненциальности функций правдоподобия классов. Теорема о линейности байесовского оптимального классификатора. Оценивание апостериорных вероятностей классов с помощью сигмоидной функции активации. Логистическая регрессия. Принцип максимума правдоподобия и логарифмическая функция потерь. Метод стохастического градиента для логарифмической функции потерь. Сглаженное правило Хэбба. Метод наименьших квадратов с итеративным пересчётом весов (IRLS). Пример прикладной задачи: кредитный скоринг. Бинаризация признаков."},{"listItemTitle":"Поиск ассоциативных правил","listItemDesc":"Понятие ассоциативного правила и его связь с понятием логической закономерности. Примеры прикладных задач: анализ рыночных корзин, выделение терминов и тематики текстов. Алгоритм APriori. Два этапа: поиск частых наборов и рекурсивное порождение ассоциативных правил. Недостатки и пути усовершенствования алгоритма APriori. Алгоритм FP-growth. Понятия FP-дерева и условного FP-дерева. Два этапа поиска частых наборов в FP-growth: построение FP-дерева и рекурсивное порождение частых наборов. Общее представление о динамических и иерархических методах поиска ассоциативных правил."}]}],"optional":[],"recommend":[],"special":[]},{"title":"Второй семестр","required":[{"title":"Машинное обучение, часть 2","description":"","teachers":"К. В. Воронцов","list":[{"listItemTitle":"Нейросетевые методы классификации и регрессии","listItemDesc":"Биологический нейрон, модель МакКаллока-Питтса как линейный классификатор. Функции активации. Проблема полноты. Задача исключающего или. Полнота двухслойных сетей в пространстве булевых функций. Теоремы Колмогорова, Стоуна, Горбаня (без доказательства). Алгоритм обратного распространения ошибок. Эвристики: формирование начального приближения, ускорение сходимости, диагональный метод Левенберга-Марквардта. Проблема \"паралича\" сети. Метод послойной настройки сети. Подбор структуры сети: методы постепенного усложнения сети, оптимальное прореживание нейронных сетей (optimal brain damage). Введение в глубинное обучение."},{"listItemTitle":"Композиционные методы классификации и регрессии","listItemDesc":"1. Линейные композиции, бустинг\n Основные понятия: базовый алгоритм (алгоритмический оператор), корректирующая операция. Взвешенное голосование. Алгоритм AdaBoost. Экспоненциальная аппроксимация пороговой функции потерь. Процесс последовательного обучения базовых алгоритмов. Теорема о сходимости бустинга. Базовые алгоритмы в бустинге. Решающие пни. Варианты бустинга: GentleBoost, LogitBoost, BrownBoost, и другие. Алгоритм AnyBoost. Градиентный бустинг.\n 2. Эвристические и стохастические методы\n Простое голосование (комитет большинства). Эвристический алгоритм ComBoost. Идентификация нетипичных объектов (выбросов). Обобщение на большое число классов. Решающий список (комитет старшинства). Эвристический алгоритм. Стратегия выбора классов для базовых алгоритмов. Стохастические методы: бэггинг и метод случайных подпространств. Случайные леса.\n3. Нелинейные алгоритмические композиции\nСмесь алгоритмов, область компетентности алгоритма. Выпуклые функции потерь. Методы построения смесей: последовательный и иерархический. Построение смеси алгоритмов с помощью EM-подобного алгоритма. Нелинейная монотонная корректирующая операция. Случай классификации. Случай регрессии. Задача монотонизации выборки, изотонная регрессия."},{"listItemTitle":"Критерии выбора моделей и методы отбора признаков","listItemDesc":"1. Задачи оценивания и выбора моделей\n Внутренние и внешние критерии. Эмпирические и аналитические оценки функционала полного скользящего контроля. Скользящий контроль, разновидности эмпирических оценок скользящего контроля. Критерий непротиворечивости. Разновидности аналитических оценок. Регуляризация. Критерий Акаике (AIC). Байесовский информационный критерий (BIC). Статистические критерии: коэффициент детерминации, критерий Фишера, анализ регрессионных остатков. Агрегированные и многоступенчатые критерии.\n 2. Теория обобщающей способности\n Теория Вапника-Червоненкиса. Функционал равномерного отклонения частот ошибок. Функция роста, ёмкость семейства алгоритмов. Структурная минимизация риска. Оценка \"бритвы Оккама\". Радемахеровская сложность семейства алгоритмов. Комбинаторная теория переобучения. Функционал вероятности переобучения. Граф расслоения-связности. Оценки расслоения-связности.\n3. Методы отбора признаков\nСложность задачи отбора признаков. Полный перебор. Метод добавления и удаления, шаговая регрессия. Поиск в глубину, метод ветвей и границ. Усечённый поиск в ширину, многорядный итерационный алгоритм МГУА. Генетический алгоритм, его сходство с МГУА. Случайный поиск и Случайный поиск с адаптацией (СПА)."},{"listItemTitle":"Ранжирование","listItemDesc":"Постановка задачи обучения ранжированию. Примеры. Признаки в задаче ранжирования поисковой выдачи: текстовые, ссылочные, кликовые. TF-IDF. PageRank. Критерии качества ранжирования: Precision, MAP, AUC, DCG, NDCG, pFound. Ранговая классификация, OC-SVM. Попарный подход: RankingSVM, RankNet, LambdaRank."},{"listItemTitle":"Обучение с подкреплением","listItemDesc":"Задача о многоруком бандите. Жадные и эпсилон-жадные стратегии. Метод UCB (upper confidence bound). Стратегия Softmax. Среда для экспериментов. Адаптивные стратегии на основе скользящих средних. Метод сравнения с подкреплением. Метод преследования. Общая постановка задачи обучения с подкреплением. Ценность состояния среды. Ценность действия. Метод временных разностей. Метод SARSA. Метод Q-обучения. Многошаговое TD-прогнозирование. Обобщения методов временных разностей, SARSA, Q-обучения. Адаптивный полужадный метод VDBE."},{"listItemTitle":"Обучение без учителя","listItemDesc":"1. Кластеризация\n Постановка задачи кластеризации. Примеры прикладных задач. Типы кластерных структур. Графовые алгоритмы кластеризации. Выделение связных компонент. Кратчайший незамкнутый путь. Алгоритм ФОРЭЛ. Функционалы качества кластеризации. Статистические алгоритмы: EM-алгоритм и Алгоритм k средних (k-means).\n 2. Сети Кохонена\n Нейронная сеть Кохонена. Конкурентное обучение, стратегии WTA и WTM. Самоорганизующаяся карта Кохонена. Применение для визуального анализа данных. Искусство интерпретации карт Кохонена. Сети встречного распространения, их применение для кусочно-постоянной и гладкой аппроксимации функций.\n 3. Таксономия\n Агломеративная кластеризация, Алгоритм Ланса-Вильямса и его частные случаи. Алгоритм построения дендрограммы. Определение числа кластеров. Свойства сжатия/растяжения, монотонности и редуктивности. Псевдокод редуктивной версии алгоритма. Потоковые (субквадратичные) алгоритмы кластеризации."},{"listItemTitle":"Задачи с частичным обучением","listItemDesc":"Постановка задачи Semisupervised Learning, примеры приложений. Простые эвристические методы: self-training, co-training, co-learning. Адаптация алгоритмов кластеризации для решения задач с частичным обучением. Кратчайшиё незамкнутый путь. Алгоритм Ланса-Уильямса. Алгоритм k-средних. Трансдуктивный метод опорных векторов TSVM. Алгоритм Expectation-Regularization на основе многоклассовой регуляризированной логистической регрессии."},{"listItemTitle":"Коллаборативная фильтрация","listItemDesc":"Задачи коллаборативной фильтрации, транзакционные данные и матрица субъекты-объекты. Корреляционные методы user-based, item-based. Латентные методы на основе би-кластеризации. Алгоритм Брегмана. Латентные методы на основе матричных разложений. Метод главных компонент для разреженных данных. Метод стохастического градиента. Неотрицательные матричные разложения. Разреженный SVD. Метод чередующихся наименьших квадратов ALS.\n Трансакционные данные. Рекомендательные системы и задачи персонализации.\n Категориальные признаки с большим множеством значений.\n Методы восстановление пропущенных значений матрицы: на основе схожести столбцов/строк, на основе предположения о низком ранге матрицы.\n Взвешенное сингулярное разложение матрицы, алгоритм ALS, распределенная реализация.\n Факторизационные машины"},{"listItemTitle":"Тематическое моделирование","listItemDesc":"Задача тематического моделирования коллекции текстовых документов. Метод максимума правдоподобия. Униграммная модель документа. Применение метода множителей Лагранжа. Вероятностный латентный семантический анализ PLSA. ЕМ-алгоритм. Латентное размещение Дирихле LDA. Сглаженная частотная оценка условной вероятности. Аддитивная регуляризация тематических моделей. Регуляризованный EM-алгоритм, теорема о стационарной точке. Регуляризаторы разреживания, сглаживания, декоррелирования тем.\n Схема построения тематической модели. Способы оценки качества тематической модели: перплексия, когерентность тем, качество поиска. Обучение тематических моделей, подбор параметров модели и процедуры обучения. Стратегии разреживания тематических моделей. Коллокации и N-граммы, их учет в тематических моделях."}]}],"optional":[{"title":"Алгоритмы и структуры данных, часть 2","description":"","teachers":"М. А. Бабенко","list":[{"listItemTitle":"Обход в ширину. Обход в глубину (начало)","listItemDesc":"Графы: основные определения, обозначения и способы хранения. Обход в ширину и его использование для нахождения кратчайших путей. Обход в глубину и его основные свойства."},{"listItemTitle":"Обход в глубину (продолжение)","listItemDesc":"Анализ учетных стоимостей операций: функция потенциала, истинные и учетные стоимости. Стеки и очереди. Реализация на основе массива переменного размера и на основе связанного списка. Моделирование очереди с помощью двух стеков. Задача о поддержании динамического максимума в стеке и очереди. Изменяемые (mutable) и неизменяемые (immutable) структуры данных. Структуры данных с хранением истории (persistent). Immutable-стек и immutable-очередь. Проблема множественного будущего при анализе учетных стоимостей в persistent-структурах."},{"listItemTitle":"Обход в глубину (окончание). 2-разрезы","listItemDesc":"Отношение взаимной достижимости вершин. Компоненты сильной связности, конденсация. Ацикличность конденсации. Поиск сильно связных компонент с помощью обхода в глубину. Топологическая сортировка конденсации. 1- и 2-разрезы. Линейное пространство на ребрах графа, его размерность. Подпространство циклов графа, его размерность и базис. Подпространство разрезов графа. Разложение пространства в прямую ортогональную сумму подпространств циклов и разрезов. Генерация случайного равновероятного элемента в пространстве циклов. Рандомизированный поиск 2-разрезов на основе fingerprints."},{"listItemTitle":"Поиск кратчайших путей (начало)","listItemDesc":"Кратчайшие пути в графе, примеры функции длин. Оценки расстояний и их релаксация. Алгоритмы Форда-Беллмана и Флойда. Алгоритм Дейкстры."},{"listItemTitle":"Поиск кратчайших путей (продолжение)","listItemDesc":"Анализ сложности алгоритма Дейкстры. Использование бинарных и k-ичных куч. Двунаправленный алгоритм Дейкстры. Системы потенциалов в задаче о кратчайших путях. Критерий консервативности функции длин дуг в терминах наличия допустимого набора потенциалов. Алгоритм Джонсона для задачи APSPP при произвольных длинах дуг. Использование маяков (landmarks) для быстрого поиска кратчайших путей. Алгоритм ALT."},{"listItemTitle":"Минимальные остовные деревья","listItemDesc":"Задача об оптимальном остовном дереве. Хорошие множества, лемма о минимальном ребре в разрезе. Алгоритмы Краскала, Прима и Борувки. Оценки сложности."},{"listItemTitle":"Минимальные разрезы. Поиск подстрок (начало)","listItemDesc":"Задачи о минимальном глобальном разрезе и о минимальном s-t разрезе, их связь. Стягиванияя графа. Алгоритм Штёра-Вагнера. Строки: основные определения и обозначения. Задача поиска подстроки в строке: различные варианты постановки. Наивный алгоритм. Префикс-функция: определение и свойства. Построение префикс-функции за линейное время. Алгоритм Кнута-Морриса-Пратта."},{"listItemTitle":"Поиск подстрок (продолжение)","listItemDesc":"Z-функция: определение и использование в задаче поиска подстроки. Построение Z-функции за линейное время. Оптимизация поиска подстрок с помощью Z-функции по памяти. Использование Z-функции для задачи приближенного поиска подстрок с одной ошибкой за линейное время. Задача множественного поиска подстрок, ожидаемая асимптотика времени работы. Бор для набора слов: определение и способы представления. Префикс-функция на боре. Алгоритм Ахо--Корасик для множественного поиска подстрок."},{"listItemTitle":"Поиск подстрок (окончание)","listItemDesc":"Поиск подстрок: джокеры типа \"?\" и \"*\". Использование алгоритма Ахо-Корасик. Оценка сложности. Редукция по символу алфавита. Поиск подходящих позиций с использованием сверток. Связь сверток с умножением полиномов. Пара оптимизаций: бинаризация алфавита и сокращение длины текста с помощью двуслойного покрытия."},{"listItemTitle":"Суффиксные деревья (начало)","listItemDesc":"Быстрое преобразование Фурье, рекурсивный вариант алгоритма. Обратное преобразование Фурье. Суффиксный бор и суффиксное дерево, оценки размеров. Явные и неявные положения в суффиксном дереве. Суффиксные ссылки."},{"listItemTitle":"Суффиксные деревья (окончание). Суффиксные массивы (начало)","listItemDesc":"Общая схема алгоритма Укконена для построения сжатого суффиксного дерева за время, линейное по длине строки. Итерации и шаги алгоритма. Классификация шагов. Лемма о возможных переходах между шагами различных типов. Элиминация шагов типа 1: неявные пометки листовых дуг. Элиминация шагов типа 3: досрочное окончание итерации. Оценка количества шагов типа 2. Поиск положений для шагов типа 2: суффиксные ссылки. Прием «скачок по счетчику» для быстрого вычисления суффиксных ссылок. Лемма об изменении вершинной глубины при переходе по суффиксной ссылке. Доказательство линейности времени работы алгоритма Укконена. Суффиксные массивы. Поиск подстрок с помощью суффиксных массивов и бинарного поиска. Построение суффиксного массива за время O(n log n) методом маркировки (алгоритм Карпа-Миллера-Розенберга)."},{"listItemTitle":"Суффиксные массивы (окончание)","listItemDesc":"Ускорение поиска подстрок с помощью предварительно рассчитанных lcp-значенеий. Алгоритм Каркайнена-Сандерса построения суффиксного массива за линейное время. Наименьшие общие предки в суффиксном дереве, связь с LCP-функцией. LCP-массив: определение и свойства. Построение LCP-массива по суффиксному массиву за линейное время (алгоритм Арикавы-Аримуры-Касаи-Ли-Парка)."},{"listItemTitle":"Длиннейшие общие подстроки. Приближенный поиск подстрок.","listItemDesc":"Задача о длиннейшей общей подстроке. Решение за линейное время с помощью суффиксного дерева и суффиксного массива. Задача приближенного поиска подстрок в тексте. Формулировка в терминах расстояний по графу динамического программирования. Алгоритм Ландау-Вишкина: множества достижимых вершин и их границы. База и шаг алгоритма, использование LCP."}]},{"title":"Язык Python","description":"","teachers":"И. В. Белова","list":[{"listItemTitle":"Основы языка (часть 1)","listItemDesc":"Интерпретатор и его интерактивный режим. Динамическая типизация, базовые типы данных: числовые, str, list. Основные операторы, оператор print. Блоки кода, основные составные операторы: if, while, for. Основные встроенные функции. Создание пользовательских функций."},{"listItemTitle":"Основы языка (часть 2)","listItemDesc":"Выражения, приоритеты операторов. Работа с файлами. Тип dict, хэширование. Модули, оператор import, модуль sys."},{"listItemTitle":"Объектно-ориентированное программирование","listItemDesc":"Классы, объекты. Пользовательские классы, методы и члены. Конструктор класса. Перегрузка операторов. Объекты в Python."},{"listItemTitle":"Обработка ошибок","listItemDesc":"Исключения, их генерация и обработка. Пользовательские исключения. Освобождение ресурсов, менеджеры контекстов."},{"listItemTitle":"Оформление и тестирование кода","listItemDesc":"Документирование кода. Инструмент pydoc. Юнит-тестирование. Модуль unittest. Инструменты для тестирования. Инструменты pylint, pyflakes. Отладочные инструменты. Модули, создание модулей. Пространства имен. Исполнение модулей как скриптов."},{"listItemTitle":"Работа со\u0026nbsp;строками","listItemDesc":"Встроенные функции строк. Форматирование строк. Модуль string. Класс unicode, его функции. Кодировки и Unicode, кодирование файлов и исходного кода."},{"listItemTitle":"Модель памяти","listItemDesc":"Хранение объектов в памяти, сборщик мусора. Хранение объектов по ссылке и по значению. Изменяемые и неизменяемые объекты. Модуль copy."},{"listItemTitle":"Функциональное программирование","listItemDesc":"Обработка списков, функция map и др., лямбда-функции, распаковка списков и словарей. Расширенная обработка аргументов функций. Генераторы и \"ленивое\" исполнение. Управляющие исключения. Модуль itertools."},{"listItemTitle":"Обзор библиотек (часть 1)","listItemDesc":"Библиотеки для обработки аргументов командной строки. Системные библиотеки. Стандартные математические библиотеки. Регулярные выражения и модуль re."},{"listItemTitle":"Обзор библиотек (часть 2)","listItemDesc":"Библиотеки для работы с HTML/XML. Математические библиотеки: SciPy и др. Библиотека Tkinter."},{"listItemTitle":"Параллельные вычисления в Python","listItemDesc":"Многопоточные программы и GIL. Многопроцессорные программы."},{"listItemTitle":"Расширенная работа с объектами","listItemDesc":"Модификаторы доступа. Наследование, разрешение имен. Метаклассы. Объект type. Декораторы."}]},{"title":"Обучение языку C++, часть 2","description":"Вторая часть курса по С++, в которой разбираются продвинутые темы и возможности языка.","teachers":"Ф. М. Короткий","list":[{"listItemTitle":"Многопоточное программирование. Синхронизация потоков с использованием мьютексов и условных переменных.","listItemDesc":""},{"listItemTitle":"Атомарные переменные. Модель памяти С++. Примеры лок-фри структур данных.","listItemDesc":""},{"listItemTitle":"Продвинутые техники мета-программирования в С++. Метафункции, SFINAE, концепты.","listItemDesc":""},{"listItemTitle":" Конкурентное программирование, взаимодействие с сетью.","listItemDesc":""},{"listItemTitle":"Архитектура llvm. Работа с синтаксическим деревом разбора С++. Разработка инструментов для анализа С++ кода.","listItemDesc":""}]}],"recommend":[],"special":[]},{"title":"Третий семестр","required":[],"optional":[{"title":"Natural Language Processing","description":"\"NLP (Natural Language Processing) — это подмножество более широкой области AI, которая пытается научить компьютер понимать и обрабатывать сырые данные на естественном языке. Большая часть доступной сегодня информации — это не структурированные тексты. Нам как людям, конечно, не составляет труда их понять (если они на родном языке), но мы не способны обработать такое количество данных, какое могла бы обработать машина. Но как заставить машину понимать эти данные и, более того, извлекать из них какую-то информацию?\nНесколько лет назад на открытии ACL (одной из основных, если не самой главной NLP-конференции) в своей президентской речи Marti Hearst призналась, что больше не может давать студентам свое любимое упражнение. На примере HAL 9000 (один из примеров искусственного интеллекта в научной фантастике) она спрашивала студентов, что машина может делать, как HAL, а что пока нет. Сейчас это уже не такое хорошее упражнение, так как почти все из этого сейчас под силу компьютеру. Поразительно, насколько быстро развивается область и как многого мы достигли.\nВ курсе мы постараемся дать вам понять и почувствовать, что происходит в мире. Какие задачи решаются, как это происходит; как некоторые статистические подходы (которым почти полностью были посвящены курсы по NLP ещё несколько лет назад) получают новую жизнь и новую интерпретацию в нейросетях, а какие постепенно отмирают. Мы покажем, что NLP это не набор пар (задача, решение), а общие идеи, которые проникают в разные задачи и отражают некоторую общую концепцию. Вы также узнаете, что происходит на практике, когда какие подходы более применимы.\nЭто то, что мы делаем, то, что мы любим, и мы готовы поделиться этим с вами :)\"","teachers":"D. Talbot, Е. Войта","list":[{"listItemTitle":"https://lena-voita.github.io/nlp_course.html","listItemDesc":""},{"listItemTitle":"https://github.com/yandexdataschool/nlp_course ","listItemDesc":""}]},{"title":"Компьютерное зрение","description":"\"Курс посвящен методам и алгоритмам компьютерного зрения, т.е. извлечения информации из изображений и видео. Рассмотрим основы обработки изображений, классификацию изображений, поиск изображений по содержанию, распознавание лиц, сегментацию изображений. Затем поговорим про алгоритмы обработки и анализа видео. Последняя часть курса посвящена трёхмерной реконструкции. Для большинства задач будем обсуждать сущестующие нейросетевые модели.\nВ курсе мы стараемся уделять внимание только наиболее современным методам, которые используются в настоящее время при решении практических и исследовательских задач. Курс в большей степени является практическим, а не теоретическим. Поэтому все лекции снабжены лабораторными и домашними работами, которые позволяют попробовать на практике большинство из рассматриваемых методов. Работы выполняются на языке Python, с использованием различных библиотек.\"","teachers":"А. С. Конушин","list":[{"listItemTitle":"Цифровое изображение и тональная коррекция","listItemDesc":""},{"listItemTitle":"Основы обработки изображений","listItemDesc":""},{"listItemTitle":"Совмещение изображений","listItemDesc":""},{"listItemTitle":"Классификация изображений и поиск похожих","listItemDesc":""},{"listItemTitle":"Сверточные нейросети для классификации и поиска похожих изображений","listItemDesc":""},{"listItemTitle":"Детектирование объектов","listItemDesc":""},{"listItemTitle":"Семантическая сегментация","listItemDesc":""},{"listItemTitle":"Перенос стиля и синтез изображений","listItemDesc":""},{"listItemTitle":"Распознавание видео","listItemDesc":""},{"listItemTitle":"Разреженная трёхмерная реконструкция","listItemDesc":""},{"listItemTitle":"Плотная трёхмерная реконструкция","listItemDesc":""},{"listItemTitle":"Реконструкция по одному кадру и облакам точек, параметрические модели","listItemDesc":""}]},{"title":"Байесовские методы в машинном обучении","description":"","teachers":"Д. П. Ветров","list":[{"listItemTitle":"Байесовский подход к теории вероятностей","listItemDesc":"Классический (частотный) и байесовские подходы к теории вероятностей. Различия в интерпретации случайности и методе вывода (статистического оценивания параметров). Байесовское обобщение логического вывода. Примеры байесовских рассуждений."},{"listItemTitle":"Аналитический байесовский вывод","listItemDesc":"Сопряженные распределения. Получение сопряженного семейства по заданной функции правдоподобия на примере многомерной гауссианы с неизвестными параметрами. Экспоненциальный класс распределений. Вид сопряженного семейства распределений к заданному семейству из экспоненциального класса. Достаточные статистики. Вывод аналитических формулы для получения апостериорного распределения."},{"listItemTitle":"Байесовский способ выбора модели","listItemDesc":"Виды вероятностных моделей. Задачи выбора модели. Примеры. Различные подходы к решению. Бритва Оккама. Обоснованность модели. Геометрический смысл обоснованности. Выбор наиболее обоснованной вероятностной модели на примере парадокса Симпсона."},{"listItemTitle":"Автоматическое определение релевантности","listItemDesc":"Проверка статистических гипотез с помощью принципа наибольшей обоснованности. Соотношение с классическими методами проверки гипотез. Обобщенная линейная регрессия. Регуляризация как способ предотвращения переобучения. Настройка индивидуальных коэффициентов регуляризации с помощью максимизации обоснованности. Эффект автоопределения значимости (релевантности)."},{"listItemTitle":"Метод релевантных векторов для задачи классификации","listItemDesc":"Задача классификации. Логистическая регрессия, метод ее обучения. Приближение обоснованности модели логистической регрессии с помощью метода Лапласа. Возможные расширения метода релевантных векторов."},{"listItemTitle":"Вероятностные модели с латентными переменными","listItemDesc":"Обучение при скрытых переменных. Разложение неполного правдоподобия. Получение ЕМ-алгоритма в общем виде. Байесовское обобщение метода главных компонент, настройка обоснованности с помощью ЕМ-алгоритма. Расширения байесовской модели главных компонент."},{"listItemTitle":"Вариационный байесовский вывод","listItemDesc":"Дивергенция Кульбака Лейблера, ее основные свойства. Итерационные формулы для минимизации прямой дивергенции в рамках факторизованного приближения. Вариационный ЕМ-алгоритм. Общая схема различных подходов к оценке скрытых переменных в вероятностных моделях."},{"listItemTitle":"Байесовская модель разделения смеси гауссиан","listItemDesc":"Задача разделения смеси распределений как задача со скрытыми переменными. Вариационное приближение для оценки апостериорного распределения для смеси гауссиан."},{"listItemTitle":"Методы Монте-Карло с марковскими цепями","listItemDesc":"Методы получения случайных величин из заданного распределения. Генерация одномерных случайных величин методом обратной функции. Выборка с отказами. Выборка с перевзвешиванием. Основные понятия теории марковских цепей. Эргодичность. Уравнение детального баланса. Схема Метрополиса-Гастингса и схема Гиббса."},{"listItemTitle":"Латентное размещение Дирихле","listItemDesc":"Задача тематического моделирования текстов. Вероятностная модель порождения текстов. Обучение модели латентного размещения Дирихле с помощью вариационного ЕМ-алгоритма. Расширения модели латентного размещения Дирихле."},{"listItemTitle":"Гауссовские процессы для регрессии и классификации","listItemDesc":"Понятие о случайных процессах. Ковариационная функция. Гауссовские случайные процессы. Автоматический подбор ковариационной функции под обучающую выборку с помощью максимизации обоснованности модели. Применение гауссовских случайных процессов в задачах классификации."},{"listItemTitle":"Непараметрические байесовские методы","listItemDesc":"Случайные процесс Дирихле как случайная мера. Модель разделения дискретной смеси на основе процесса Дирихле. Применение процесса Дирихле в непараметрической кластеризации. Схема китайского ресторана и стик-брейкинга. Дальнейшие расширения модели процессов Дирихле (вложенные процессы Дирихле, иерархические процессы Дирихле). Примеры применения процессов Дирихле для решения прикладных задач."}]}],"recommend":[],"special":[]},{"title":"Четвёртый семестр","required":[{"title":"ML Engineering Practice","description":"Курс представляет собой проектную работу по разработке ML-проектов в командах.","teachers":"Ф.Г. Синицин","list":[]},{"title":"ML Research Practice","description":"Курс представляет собой работу над командными исследовательскими проектам в области машинного обучения.","teachers":"","list":[]}],"optional":[],"recommend":[{"title":"Глубинное обучение","description":"","teachers":"В. С. Лемпицкий","list":[{"listItemTitle":"[Материал курса] (https://github.com/yandexdataschool/Practical_DL)","listItemDesc":""}]},{"title":"Обучение с подкреплением","description":"","teachers":"Ф. Д. Ратников","list":[{"listItemTitle":"[Материал курса] (https://github.com/yandexdataschool/Practical_RL)","listItemDesc":""}]},{"title":"Self Driving Cars","description":"В курсе рассматриваются основные компоненты беспилотных технологий: локализация, перцепция, предсказание, уровень поведения и планирование движения. Для каждой из компонент будут описаны основные подходы. Кроме того, студенты познакомятся с текущим состоянием рынка и технологическими вызовами.","teachers":"А. В. Слесарев","list":[{"listItemTitle":"Обзор основных компонент и сенсоров беспилотного автомобиля. Уровни автономности. Drive by Wire. Беспилотные автомобили как бизнес-продукт. Способы оценки прогресса в создании беспилотников. Основы локализации: gnss, колесная одометрия, байесовские фильтры.","listItemDesc":""},{"listItemTitle":"Методы лидарной локализации: ICP, NDT, LOAM. Введение в визуальный SLAM на примере ORB-SLAM. Постановка задачи GraphSLAM. Сведение задачи GraphSLAM к нелинейному МНК. Выбор правильной параметризации. Системы с особой структурой в GraphSLAM. Архитектурный подход: frontend и backend.","listItemDesc":""},{"listItemTitle":"Задача распознавания в беспилотном автомобиле. Статические и динамические препятствия. Сенсоры для системы распознавания. Представление статических препятствий. Детекция статических препятствий по лидару (VSCAN, нейросетевые методы). Использование лидара совместно с изображениями для детекции статики (семантическая сегментация изображений, depth completion). Стерео камера и получение глубины из картинки. Stixel World.","listItemDesc":""},{"listItemTitle":"Представление динамических препятствий в беспилотном автомобиле. Нейросетевые методы детекции объектов в 2D. Детекция на основе Bird-eye view представления лидарного облака. Использование лидара совместно с изображениями для детекции динамических препятствий. Детекция автомобилей в 3D на основе картинок (3D boxes fitting, CAD models). Детекция динамических препятствий на основе радара. Трекинг объектов.","listItemDesc":""},{"listItemTitle":"Модели движения автомобиля: rear wheel, front wheel. Планирование траекторий. Понятие конфигурационного пространства. Графовые методы построения траекторий. Траектории, минимизирующие рывок. Оптимизационные методы построения траекторий.","listItemDesc":""},{"listItemTitle":"Планирование скорости в динамическом окружении. ST-планирование. Предсказание поведения других участников дорожного движения","listItemDesc":""}]},{"title":"Нейробайесовские методы","description":"Курс посвящен применению байесовских методов в глубинном обучении. На лекциях будет рассказано о применении вероятностного моделирования для построения порождающих моделей данных, использованию состязающихся сетей для приближенного вывода, моделированию неопределенности в параметрах нейронной сети и о некоторых открытых проблемах глубинного обучения.","teachers":"Д. П. Ветров","list":[{"listItemTitle":"Стохастический вариационный вывод","listItemDesc":""},{"listItemTitle":"Дважды стохастический вариационный вывод","listItemDesc":""},{"listItemTitle":"Вариационный автокодировщик, нормализующие потоки для вариационного вывода","listItemDesc":""},{"listItemTitle":"Методы снижения дисперсии в моделях со скрытыми переменными","listItemDesc":""},{"listItemTitle":"Оценка отношения плотностей распределений, применение на примере \\alpha-GAN","listItemDesc":""},{"listItemTitle":"Байесовские нейронные сети","listItemDesc":""},{"listItemTitle":"Байесовское сжатие нейронных сетей","listItemDesc":""},{"listItemTitle":"Полунеявный вариационный вывод","listItemDesc":""}]}],"special":[]}]},"seo":{"title":"Разработка машинного обучения","keywords":"","description":""}},"menu":{"title":"Специальности","menu":[{"page":"data-science","title":"Data Science","url":"/dataschool/course/data-science","text":"Решение задач по сбору и\u0026nbsp;анализу данных, возникающих в\u0026nbsp;большинстве современных сервисов: от\u0026nbsp;Алисы до\u0026nbsp;Яндекс.Погоды."},{"page":"machine-learning","title":"Разработка машинного обучения","url":"/dataschool/course/machine-learning","text":"Создание высокотехнологичных сервисов и\u0026nbsp;приложений на\u0026nbsp;основе машинного обучения."},{"page":"big-data-infrastructure","title":"Инфраструктура больших данных","url":"/dataschool/course/big-data-infrastructure","text":"Разработка систем хранения и\u0026nbsp;обработки больших данных."},{"page":"data-analysis","title":"Анализ данных в\u0026nbsp;прикладных науках","url":"/dataschool/course/data-analysis","text":"Применение data science в\u0026nbsp;областях, напрямую не связанных с IT: от\u0026nbsp;физики высоких энергий до\u0026nbsp;промышленного дизайна лекарств."},{"page":"alternative-track","title":"Альтернативный трек","url":"/dataschool/course/alternative-track","text":"Тем, у кого есть опыт в IT, но нет сильной математической базы, альтернативный трек поможет добрать недостающие знания по математике и влиться в одно из четырёх других направлений"}]}},"common":{"options":{"seo":{"title":"Школа анализа данных","description":"","keywords":"","verification":"e52884e5ba57aa72","thumbnail":"social-share.ru.png","metrikaID":550285}},"header":{"logoPrimaryTitle":{"home":{"full":"Школа анализа данных","staticPart":"Школа анализа данных","fullReplacement":"","shortReplacement":""},"inner":{"full":"Школа анализа данных","staticPart":"ШАД","fullReplacement":"","shortReplacement":""}},"title":"Меню","menu":[{"name":"home","title":"Главная","url":"/dataschool/","sideMenu":true},{"name":"enroll","title":"Поступающим","url":"/dataschool/enroll","sideMenu":true},{"name":"education","title":"Учёба","url":"/dataschool/education","sideMenu":true},{"name":"universities","title":"Вузы","url":"/dataschool/universities","sideMenu":true},{"name":"life","title":"Жизнь","url":"/dataschool/life","sideMenu":true},{"name":"science","title":"Наука","url":"/dataschool/science","sideMenu":true},{"name":"online","title":"Учебник и онлайн-курсы","url":"/dataschool/online","sideMenu":true},{"name":"documents","title":"Сведения об образовательной организации","url":"/dataschool/documents ","sideMenu":false}]},"footer":{"copyright":"\u0026copy; Школа анализа данных, 2007\u0026nbsp;\u0026mdash;\u0026nbsp;2022","logo":{"title":"Яндекс","link":"https://yandex.ru"},"email":"shad@yandex-team.ru","feedback":{"title":""}}},"id":"machine-learning"},"buildId":"IXSTiFLdDRijwQc4qj1CK","isFallback":false,"customServer":true,"appGip":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-011e37378a3d76f1f31b.js"></script><script src="/_next/static/chunks/main-6d2a64d1b2609e3ff0c4.js" async=""></script><script src="/_next/static/chunks/webpack-a7c69ae57fb2c90b0ff0.js" async=""></script><script src="/_next/static/chunks/framework.5e03480598ad1120f007.js" async=""></script><script src="/_next/static/chunks/commons.fd125b3506c044f87988.js" async=""></script><script src="/_next/static/chunks/pages/_app-8d36acfba5cc49432a11.js" async=""></script><script src="/_next/static/chunks/36a89214.0631567be2ca53cd9149.js" async=""></script><script src="/_next/static/chunks/ea88be26.7653e1aac39d91071425.js" async=""></script><script src="/_next/static/chunks/c548a0b8041c04ca6c11df514919242021982897.f6d5fe8a9488ddf15b3e.js" async=""></script><script src="/_next/static/chunks/c548a0b8041c04ca6c11df514919242021982897_CSS.e1033ba52a9d5b03d3fc.js" async=""></script><script src="/_next/static/chunks/f0811fe9cf5fab03b83c5f353f05045bc09b9dd7.5cd3f8f8c52901791310.js" async=""></script><script src="/_next/static/chunks/f0811fe9cf5fab03b83c5f353f05045bc09b9dd7_CSS.f747071f9f407dc1e291.js" async=""></script><script src="/_next/static/chunks/styles.51b0d17bb8fb84a0f8d2.js" async=""></script><script src="/_next/static/chunks/pages/course/%5Bid%5D-70b94a5e5d4d5db3c66f.js" async=""></script><script src="/_next/static/IXSTiFLdDRijwQc4qj1CK/_buildManifest.js" async=""></script><script src="/_next/static/IXSTiFLdDRijwQc4qj1CK/_ssgManifest.js" async=""></script></body></html>